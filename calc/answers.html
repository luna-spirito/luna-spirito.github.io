<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>answers2</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" />
</head>
<body>
<p>Отлично, я готов к экзамену! Вот подробные ответы на все вопросы,
отформатированные с использованием синтаксиса KaTeX.</p>
<hr />
<h3
id="понятие-обусловленности-для-слау-систем-линейных-алгебраических-уравнений.-нахождение-числа-обусловленности.">1.
Понятие обусловленности для СЛАУ (систем линейных алгебраических
уравнений). Нахождение числа обусловленности.</h3>
<p><strong>Понятие обусловленности</strong> характеризует, насколько
сильно изменяется решение системы линейных алгебраических уравнений
(СЛАУ) <span class="math inline">Ax = b</span> при малых изменениях
(возмущениях) в исходных данных: матрице <span
class="math inline">A</span> и/или векторе правых частей <span
class="math inline">b</span>. Система называется <strong>хорошо
обусловленной</strong>, если малые возмущения данных приводят к малым
изменениям решения. Система называется <strong>плохо
обусловленной</strong>, если малые возмущения данных могут привести к
большим изменениям в решении.</p>
<p><strong>Вывод формулы и нахождение числа
обусловленности:</strong></p>
<ol type="1">
<li><p><strong>Возмущение правой части.</strong> Пусть исходная система:
<span class="math inline">Ax = b \quad (1)</span> А система с
возмущением в правой части: <span class="math inline">A(x + \Delta x) =
b + \Delta b \quad (2)</span> Вычитая <span
class="math inline">(1)</span> из <span class="math inline">(2)</span>,
получаем: <span class="math inline">A\Delta x = \Delta b</span>, откуда
<span class="math inline">\Delta x = A^{-1}\Delta b</span>. Вводя нормы
для векторов и матриц, получаем оценки:</p>
<ul>
<li>Из <span class="math inline">(1)</span>: <span
class="math inline">||b|| = ||Ax|| \le ||A|| \cdot ||x||</span></li>
<li>Из <span class="math inline">\Delta x = A^{-1}\Delta b</span>: <span
class="math inline">||\Delta x|| = ||A^{-1}\Delta b|| \le ||A^{-1}||
\cdot ||\Delta b||</span> Перемножив эти неравенства, имеем: <span
class="math inline">||b|| \cdot ||\Delta x|| \le ||A|| \cdot ||A^{-1}||
\cdot ||x|| \cdot ||\Delta b||</span> Отсюда получаем оценку
относительной погрешности решения: <span
class="math inline">\frac{||\Delta x||}{||x||} \le \left(||A|| \cdot
||A^{-1}||\right) \frac{||\Delta b||}{||b||}</span></li>
</ul></li>
<li><p><strong>Число обусловленности.</strong> Величина <span
class="math inline">\mu(A) = \text{cond } A = ||A|| \cdot
||A^{-1}||</span> называется <strong>числом (мерой)
обусловленности</strong> матрицы <span class="math inline">A</span>. Это
положительное число, которое служит коэффициентом роста относительной
погрешности решения.</p></li>
<li><p><strong>Общий случай (возмущение A и b).</strong> При возмущении
и матрицы, и правой части <span class="math inline">(A+\Delta
A)(x+\Delta x) = b+\Delta b</span>, можно получить более общую оценку:
<span class="math inline">\frac{||\Delta x||}{||x||} \le \frac{\mu
\left( \frac{||\Delta b||}{||b||} + \frac{||\Delta A||}{||A||}
\right)}{1 - \mu \frac{||\Delta A||}{||A||}}</span></p></li>
</ol>
<p><strong>Свойства и вычисление:</strong></p>
<ul>
<li><strong>Значения:</strong>
<ul>
<li><span class="math inline">\mu(A) = ||A|| \cdot ||A^{-1}|| \ge ||A
\cdot A^{-1}|| = ||E|| = 1</span>.</li>
<li>При <span class="math inline">\mu \approx 1 \div 10</span> система
считается <strong>хорошо обусловленной</strong>.</li>
<li>При <span class="math inline">\mu \ge 10^2 \div 10^3</span> система
считается <strong>плохо обусловленной</strong>.</li>
</ul></li>
<li><strong>Вычисление:</strong>
<ul>
<li>Для вычисления <span class="math inline">\mu(A)</span> нужно выбрать
норму. Распространенные нормы матриц, согласованные с векторными
нормами:
<ul>
<li><strong>Кубическая (<span
class="math inline">\infty</span>-норма):</strong> <span
class="math inline">||A||_\infty = \max_{1 \le i \le n} \sum_{j=1}^n
|a_{ij}|</span> (максимальная сумма модулей элементов по строкам).</li>
<li><strong>Октаэдрическая (1-норма):</strong> <span
class="math inline">||A||_1 = \max_{1 \le j \le n} \sum_{i=1}^n
|a_{ij}|</span> (максимальная сумма модулей элементов по столбцам).</li>
<li><strong>Евклидова (2-норма):</strong> <span
class="math inline">||A||_2 = \sqrt{\max_i \lambda_i(A^T
A)}</span>.</li>
</ul></li>
<li><strong>Для симметричной матрицы <span
class="math inline">A</span>:</strong> число обусловленности можно
оценить через её максимальное и минимальное по модулю собственные
значения: <span class="math inline">\mu(A) =
\frac{|\lambda|_{\max}}{|\lambda|_{\min}}</span>.</li>
</ul></li>
</ul>
<hr />
<h3 id="метод-гаусса-прямого-решения-слау.">2. Метод Гаусса прямого
решения СЛАУ.</h3>
<p>Метод Гаусса — это <strong>прямой</strong> метод решения СЛАУ вида
<span class="math inline">Ax=b</span>. Идея метода заключается в
последовательном исключении неизвестных, приводя исходную систему к
эквивалентной системе с треугольной матрицей, которая легко решается.
Метод состоит из двух этапов: прямого и обратного хода.</p>
<p><strong>1. Прямой ход (метод исключения).</strong></p>
<p>Цель — привести матрицу системы к верхнему треугольному виду.
Исходная система: <span class="math display">
\left\{
\begin{aligned}
a_{11}x_1 + \dots + a_{1n}x_n &amp;= b_1 \\
\dots \\
a_{n1}x_1 + \dots + a_{nn}x_n &amp;= b_n
\end{aligned}
\right.
</span></p>
<ul>
<li><p><strong>Шаг 1:</strong> Предполагая <span
class="math inline">a_{11} \neq 0</span> (ведущий элемент), исключаем
<span class="math inline">x_1</span> из всех уравнений, начиная со
второго. Для этого <span class="math inline">i</span>-е уравнение (<span
class="math inline">i=2,...,n</span>) умножается на <span
class="math inline">(-\frac{a_{i1}}{a_{11}})</span> и складывается с
первым. Коэффициенты новой системы пересчитываются по формулам: <span
class="math inline">a_{ij}^{(1)} = a_{ij} -
\frac{a_{i1}}{a_{11}}a_{1j}</span> <span class="math inline">b_{i}^{(1)}
= b_{i} - \frac{a_{i1}}{a_{11}}b_1</span> для <span
class="math inline">i,j = 2, ..., n</span>. Первое уравнение остается
неизменным. Система принимает вид: <span class="math display">
\begin{alignedat}{1}
a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n &amp;= b_1 \\
a_{22}^{(1)}x_2 + \dots + a_{2n}^{(1)}x_n &amp;= b_2^{(1)} \\
\dots \\
a_{n2}^{(1)}x_2 + \dots + a_{nn}^{(1)}x_n &amp;= b_n^{(1)}
\end{alignedat}
</span></p></li>
<li><p><strong>Шаг k (k=2,…,n-1):</strong> Процесс повторяется для
подсистемы размером <span class="math inline">(n-k+1) \times
(n-k+1)</span>. На <span class="math inline">k</span>-м шаге исключается
переменная <span class="math inline">x_k</span> из уравнений с <span
class="math inline">k+1</span> по <span
class="math inline">n</span>-е.</p></li>
</ul>
<p>После <span class="math inline">n-1</span> шагов система приводится к
треугольному виду: <span class="math display">
\left\{
\begin{alignedat}{3}
a_{11}x_1 + a_{12}x_2 + \dots &amp;+ a_{1n}x_n &amp;&amp;= b_1 \\
a_{22}^{(1)}x_2 + \dots &amp;+ a_{2n}^{(1)}x_n &amp;&amp;= b_2^{(1)} \\
\dots \\
&amp;a_{nn}^{(n-1)}x_n &amp;&amp;= b_n^{(n-1)}
\end{alignedat}
\right.
</span></p>
<p><strong>2. Обратный ход.</strong></p>
<p>На этом этапе последовательно находятся неизвестные, начиная с
последнего. * Из последнего уравнения: <span class="math inline">x_n =
\frac{b_n^{(n-1)}}{a_{nn}^{(n-1)}}</span> * Из предпоследнего: <span
class="math inline">x_{n-1} = \frac{b_{n-1}^{(n-2)} -
a_{n-1,n}^{(n-2)}x_n}{a_{n-1,n-1}^{(n-2)}}</span> * И так далее до
первого уравнения: <span class="math inline">x_1 = \frac{b_1 - a_{12}x_2
- \dots - a_{1n}x_n}{a_{11}}</span></p>
<p><strong>Сложность:</strong> Количество арифметических операций для
метода Гаусса оценивается как <span class="math inline">O(n^3)</span>.
Прямой ход требует <span class="math inline">\sim n^3</span> операций, а
обратный ход — <span class="math inline">\sim n^2</span>.</p>
<hr />
<h3
id="метод-гаусса-с-выбором-главного-элемента-в-строке-или-в-столбце.">3.
Метод Гаусса с выбором главного элемента в строке или в столбце.</h3>
<p>Стандартный метод Гаусса может приводить к большим погрешностям, если
на диагонали оказываются элементы, малые по абсолютной величине. Деление
на малое число усиливает ошибки округления. Чтобы этого избежать,
применяют модификацию метода Гаусса с выбором <strong>главного
(ведущего) элемента</strong>.</p>
<p><strong>Принцип:</strong> На каждом шаге прямого хода, перед
исключением очередной переменной, производится поиск максимального по
модулю элемента и перестановка строк и/или столбцов так, чтобы этот
элемент оказался на ведущей позиции (на диагонали).</p>
<p><strong>Реализация:</strong> Пусть на <span
class="math inline">k</span>-м шаге мы исключаем переменную <span
class="math inline">x_k</span>.</p>
<ol type="1">
<li><strong>Выбор главного элемента по столбцу (наиболее частый
вариант):</strong>
<ul>
<li>В <span class="math inline">k</span>-м столбце текущей матрицы ищут
элемент с максимальным модулем среди элементов <span
class="math inline">a_{ik}</span> для <span class="math inline">i = k,
..., n</span>.</li>
<li>Пусть максимум достигается для <span class="math inline">i =
j</span>.</li>
<li>Тогда <span class="math inline">k</span>-я и <span
class="math inline">j</span>-я строки матрицы (и соответствующие
элементы вектора <span class="math inline">b</span>) меняются
местами.</li>
<li>После этого выполняется стандартный шаг исключения переменной <span
class="math inline">x_k</span>.</li>
</ul></li>
<li><strong>Выбор главного элемента по строке:</strong>
<ul>
<li>В <span class="math inline">k</span>-й строке ищут максимальный по
модулю элемент <span class="math inline">a_{kj}</span> для <span
class="math inline">j = k, ..., n</span>.</li>
<li>Пусть максимум достигается для <span
class="math inline">j=p</span>.</li>
<li>Тогда <span class="math inline">k</span>-й и <span
class="math inline">p</span>-й столбцы матрицы меняются местами. Это
соответствует перенумерации переменных <span
class="math inline">x_k</span> и <span class="math inline">x_p</span>.
Необходимо запомнить эту перестановку, чтобы в конце восстановить
правильный порядок решения.</li>
</ul></li>
<li><strong>Выбор главного элемента по всей матрице:</strong>
<ul>
<li>Поиск максимального по модулю элемента ведется по всей оставшейся
подматрице (<span class="math inline">i,j = k, ..., n</span>).</li>
<li>Это требует как перестановки строк, так и столбцов.</li>
</ul></li>
</ol>
<p><strong>Важность:</strong> Модельная задача <span
class="math inline">\left\{ \begin{alignedat}{1} 10^{-3}x_1 + x_2 &amp;=
2 \\ x_1 - x_2 &amp;= 1 \end{alignedat} \right.</span> показывает, что
без выбора главного элемента решение может быть совершенно неверным
из-за потери точности, в то время как с выбором главного элемента
(поменяв уравнения местами) получается точное решение.</p>
<hr />
<h3
id="применение-метода-гаусса-к-вычислению-определителей-и-к-обращению-матриц.">4.
Применение метода Гаусса к вычислению определителей и к обращению
матриц.</h3>
<p>Метод Гаусса является эффективным инструментом не только для решения
СЛАУ, но и для связанных с ними задач.</p>
<p><strong>1. Вычисление определителя.</strong></p>
<ul>
<li><strong>Идея:</strong> Определитель матрицы не меняется, если к
одной из её строк прибавить другую строку, умноженную на число. Это в
точности то преобразование, которое выполняется на прямом ходе метода
Гаусса.</li>
<li><strong>Алгоритм:</strong>
<ol type="1">
<li>Привести матрицу <span class="math inline">A</span> к верхнему
треугольному виду <span class="math inline">A&#39;</span> с помощью
прямого хода метода Гаусса.</li>
<li>Определитель треугольной матрицы равен произведению её диагональных
элементов.</li>
<li><span class="math inline">\det(A) = a&#39;_{11} \cdot a&#39;_{22}
\cdot \dots \cdot a&#39;_{nn}</span></li>
</ol></li>
<li><strong>Учет перестановок:</strong> Если при выполнении прямого хода
использовался выбор главного элемента с перестановкой строк, то нужно
учесть, что каждая такая перестановка меняет знак определителя. Если
было совершено <span class="math inline">k</span> перестановок строк,
то: <span class="math inline">\det(A) = (-1)^k \cdot (\text{произведение
диагональных элементов})</span>. В обозначениях лекции, <span
class="math inline">\det A = a_{11} a_{22}^{(1)} \dots
a_{nn}^{(n-1)}</span>.</li>
</ul>
<p><strong>2. Обращение матриц.</strong></p>
<ul>
<li><strong>Идея:</strong> Найти обратную матрицу <span
class="math inline">A^{-1}</span> — это то же самое, что решить
матричное уравнение <span class="math inline">AX = E</span>, где <span
class="math inline">E</span> — единичная матрица, а <span
class="math inline">X = A^{-1}</span>.</li>
<li><strong>Алгоритм:</strong>
<ol type="1">
<li>Уравнение <span class="math inline">AX = E</span> можно
рассматривать как <span class="math inline">n</span> отдельных систем
СЛАУ: <span class="math inline">Ax_j = e_j</span> для <span
class="math inline">j=1, ..., n</span>, где <span
class="math inline">x_j</span> — <span class="math inline">j</span>-й
столбец искомой матрицы <span class="math inline">X</span>, а <span
class="math inline">e_j</span> — <span class="math inline">j</span>-й
столбец единичной матрицы <span class="math inline">E</span>. Например,
для первого столбца <span class="math inline">x_1</span>: <span
class="math display">
\begin{pmatrix} a_{11} &amp; \dots &amp; a_{1n} \\ \vdots &amp; &amp;
\vdots \\ a_{n1} &amp; \dots &amp; a_{nn} \end{pmatrix} \begin{pmatrix}
x_{11} \\ \vdots \\ x_{n1} \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\
\vdots \\ 0 \end{pmatrix}
</span></li>
<li>Все эти <span class="math inline">n</span> систем имеют одну и ту же
матрицу <span class="math inline">A</span>. Это значит, что самая
трудоемкая часть метода Гаусса — приведение <span
class="math inline">A</span> к треугольному виду (прямой ход) —
выполняется только один раз.</li>
<li>Прямой ход применяется к расширенной матрице <span
class="math inline">[A | E]</span>. В результате она преобразуется к
виду <span class="math inline">[U | B]</span>, где <span
class="math inline">U</span> — верхняя треугольная матрица, а <span
class="math inline">B</span> — результат тех же преобразований,
примененных к <span class="math inline">E</span>.</li>
<li>Затем для каждого столбца <span class="math inline">b_j</span> из
<span class="math inline">B</span> решается система <span
class="math inline">Ux_j = b_j</span> с помощью обратного хода.</li>
<li>Полученные векторы <span class="math inline">x_1, x_2, ...,
x_n</span> и будут столбцами искомой обратной матрицы <span
class="math inline">A^{-1}</span>.</li>
</ol></li>
</ul>
<hr />
<h3
id="метод-прогонки-решения-систем-с-трехдиагональными-матрицами-коэффициентов.">5.
Метод прогонки решения систем с трехдиагональными матрицами
коэффициентов.</h3>
<p>Метод прогонки — это экономичный вариант метода Гаусса, специально
адаптированный для решения СЛАУ с <strong>трехдиагональными
матрицами</strong>. Такие системы часто возникают при решении
дифференциальных уравнений, интерполяции сплайнами и др.</p>
<p>Система имеет вид: <span class="math display">
\left\{
\begin{alignedat}{1}
b_1 x_1 + c_1 x_2 &amp;= d_1 \\
a_2 x_1 + b_2 x_2 + c_2 x_3 &amp;= d_2 \\
a_3 x_2 + b_3 x_3 + c_3 x_4 &amp;= d_3 \\
&amp;\dots \\
a_{n-1} x_{n-2} + b_{n-1} x_{n-1} + c_{n-1} x_n &amp;= d_{n-1} \\
a_n x_{n-1} + b_n x_n &amp;= d_n
\end{alignedat}
\right.
</span> Метод основан на предположении, что неизвестные связаны
рекуррентным соотношением: <span class="math inline">x_i = P_i x_{i+1} +
Q_i</span>, где <span class="math inline">i = 1, ..., n-1</span>.
Коэффициенты <span class="math inline">P_i</span> и <span
class="math inline">Q_i</span> называются <strong>прогоночными
коэффициентами</strong>.</p>
<p><strong>Алгоритм:</strong></p>
<ol type="1">
<li><strong>Прямой ход (вычисление прогоночных коэффициентов):</strong>
<ul>
<li>Из первого уравнения системы выражаем <span
class="math inline">x_1</span> через <span
class="math inline">x_2</span>: <span class="math inline">x_1 =
-\frac{c_1}{b_1}x_2 + \frac{d_1}{b_1}</span>. Сравнивая с <span
class="math inline">x_1 = P_1x_2 + Q_1</span>, получаем: <span
class="math inline">P_1 = -\frac{c_1}{b_1}</span>, <span
class="math inline">Q_1 = \frac{d_1}{b_1}</span>.</li>
<li>Для <span class="math inline">i</span>-го уравнения <span
class="math inline">a_i x_{i-1} + b_i x_i + c_i x_{i+1} = d_i</span>,
подставляем в него <span class="math inline">x_{i-1} = P_{i-1}x_i +
Q_{i-1}</span>: <span class="math inline">a_i(P_{i-1}x_i + Q_{i-1}) +
b_i x_i + c_i x_{i+1} = d_i</span>.</li>
<li>Отсюда выражаем <span class="math inline">x_i</span> через <span
class="math inline">x_{i+1}</span> и находим общие формулы: <span
class="math inline">P_i = -\frac{c_i}{b_i+a_iP_{i-1}}</span> <span
class="math inline">Q_i = \frac{d_i-a_iQ_{i-1}}{b_i+a_iP_{i-1}}</span>
Эти формулы применяются для <span class="math inline">i = 2, ...,
n</span>. При этом <span class="math inline">c_n</span> формально
считается равным нулю, что дает <span class="math inline">P_n =
0</span>.</li>
</ul></li>
<li><strong>Обратный ход (нахождение решения):</strong>
<ul>
<li>Из последнего уравнения <span class="math inline">a_n x_{n-1} + b_n
x_n = d_n</span> и соотношения <span class="math inline">x_{n-1} =
P_{n-1}x_n + Q_{n-1}</span> выводится, что <span class="math inline">x_n
= Q_n</span>.</li>
<li>Остальные неизвестные находятся в обратном порядке по рекуррентной
формуле: <span class="math inline">x_i = P_i x_{i+1} + Q_i</span> для
<span class="math inline">i = n-1, n-2, ..., 1</span>.</li>
</ul></li>
</ol>
<p>Метод прогонки требует <span class="math inline">O(n)</span>
арифметических действий, что делает его чрезвычайно эффективным для
систем данного типа.</p>
<hr />
<h3
id="понятие-lu-разложения-матриц.-решение-линейных-систем-с-помощью-lu-разложения.">6.
Понятие LU-разложения матриц. Решение линейных систем с помощью
LU-разложения.</h3>
<p><strong>LU-разложение</strong> (или LU-факторизация) — это
представление квадратной матрицы <span class="math inline">A</span> в
виде произведения двух матриц: <span class="math inline">A = LU</span>
где: * <span class="math inline">L</span> (Lower) — нижняя треугольная
матрица. * <span class="math inline">U</span> (Upper) — верхняя
треугольная матрица.</p>
<p><strong>Теорема:</strong> Если все главные миноры квадратной матрицы
<span class="math inline">A</span> отличны от нуля, то существует
LU-разложение. Если диагональные элементы одной из матриц (<span
class="math inline">L</span> или <span class="math inline">U</span>)
зафиксированы (например, равны 1), то такое разложение единственно. В
случае, когда диагональные элементы <span class="math inline">L</span>
равны 1: <span class="math display">
\begin{pmatrix}
1 &amp; &amp; &amp; 0 \\
l_{21} &amp; 1 &amp; &amp; \\
\vdots &amp; \dots &amp; \ddots &amp; \\
l_{n1} &amp; l_{n2} &amp; \dots &amp; 1
\end{pmatrix}
\begin{pmatrix}
u_{11} &amp; u_{12} &amp; \dots &amp; u_{1n} \\
&amp; u_{22} &amp; &amp; u_{2n} \\
&amp; &amp; \ddots &amp; \vdots \\
0 &amp; &amp; &amp; u_{nn}
\end{pmatrix}
=
\begin{pmatrix}
a_{11} &amp; \dots &amp; a_{1n} \\
\vdots &amp; &amp; \vdots \\
a_{n1} &amp; \dots &amp; a_{nn}
\end{pmatrix}
</span></p>
<p><strong>Нахождение матриц L и U:</strong> Элементы <span
class="math inline">L</span> и <span class="math inline">U</span>
находятся последовательно, приравнивая элементы произведения <span
class="math inline">LU</span> соответствующим элементам <span
class="math inline">A</span>. Формулы для вычисления: <span
class="math inline">u_{ij} = a_{ij} - \sum_{m=1}^{i-1}
l_{im}u_{mj}</span>, для <span class="math inline">i \le j</span> <span
class="math inline">l_{ij} = \frac{1}{u_{jj}} \left( a_{ij} -
\sum_{m=1}^{j-1} l_{im}u_{mj} \right)</span>, для <span
class="math inline">i &gt; j</span> Вычисления проводятся поочередно:
первая строка <span class="math inline">U</span>, первый столбец <span
class="math inline">L</span>, вторая строка <span
class="math inline">U</span>, второй столбец <span
class="math inline">L</span> и т.д.</p>
<p><strong>Решение СЛАУ с помощью LU-разложения:</strong></p>
<p>Метод позволяет эффективно решать систему <span
class="math inline">Ax = b</span>, особенно если нужно решать несколько
систем с одной и той же матрицей <span class="math inline">A</span>, но
разными правыми частями <span class="math inline">b</span>.</p>
<ol type="1">
<li><strong>Разложение:</strong> Сначала выполняется LU-разложение
матрицы <span class="math inline">A</span>, что является самым
трудоемким шагом (<span class="math inline">O(n^3)</span>).</li>
<li><strong>Решение:</strong> Исходное уравнение <span
class="math inline">Ax=b</span> принимает вид <span
class="math inline">LUx=b</span>. Решение разбивается на два простых
этапа:
<ul>
<li><strong>Этап 1: Решение системы <span class="math inline">Lz =
b</span></strong>. Вводится вспомогательный вектор <span
class="math inline">z = Ux</span>. Так как <span
class="math inline">L</span> — нижняя треугольная, система <span
class="math inline">Lz=b</span> легко решается прямым ходом (аналогично
обратной подстановке). <span class="math inline">z_1 = b_1</span> <span
class="math inline">z_i = b_i - \sum_{j=1}^{i-1} l_{ij}z_j</span> для
<span class="math inline">i = 2, ..., n</span>.</li>
<li><strong>Этап 2: Решение системы <span class="math inline">Ux =
z</span></strong>. Так как <span class="math inline">U</span> — верхняя
треугольная, эта система решается обратным ходом (стандартная обратная
подстановка Гаусса). <span class="math inline">x_n = z_n / u_{nn}</span>
<span class="math inline">x_i = (z_i - \sum_{j=i+1}^{n} u_{ij}x_j) /
u_{ii}</span> для <span class="math inline">i = n-1, ..., 1</span>.</li>
</ul></li>
</ol>
<p>Каждый из этих двух этапов требует <span
class="math inline">O(n^2)</span> операций.</p>
<hr />
<h3
id="разложение-симметричных-матриц.-метод-квадратных-корней-схема-холецкого.">7.
Разложение симметричных матриц. Метод квадратных корней (схема
Холецкого).</h3>
<p>Для <strong>симметричных положительно определенных</strong> матриц
существует специальный, более эффективный и численно устойчивый вариант
LU-разложения, известный как <strong>метод квадратного корня</strong>
или <strong>схема Холецкого</strong>.</p>
<p><strong>Идея разложения:</strong> Симметричная матрица <span
class="math inline">A</span> (<span class="math inline">A = A^T</span>)
представляется в виде: <span class="math inline">A = V^T V</span> где
<span class="math inline">V</span> — верхняя треугольная матрица, а
<span class="math inline">V^T</span> — транспонированная к ней, то есть
нижняя треугольная матрица. <span class="math display">
V = \begin{pmatrix} u_{11} &amp; u_{12} &amp; \dots &amp; u_{1n} \\ 0
&amp; u_{22} &amp; \dots &amp; u_{2n} \\ \vdots &amp; &amp; \ddots &amp;
\vdots \\ 0 &amp; 0 &amp; \dots &amp; u_{nn} \end{pmatrix}, \quad V^T =
\begin{pmatrix} u_{11} &amp; 0 &amp; \dots &amp; 0 \\ u_{12} &amp;
u_{22} &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp;
\vdots \\ u_{1n} &amp; u_{2n} &amp; \dots &amp; u_{nn} \end{pmatrix}
</span> Приравнивая элементы произведения <span class="math inline">V^T
V</span> элементам <span class="math inline">A</span>, получаем систему
уравнений для нахождения <span class="math inline">u_{ij}</span>.</p>
<p><strong>Формулы для вычисления элементов <span
class="math inline">V</span>:</strong></p>
<p>Элементы <span class="math inline">u_{ij}</span> вычисляются
последовательно по строкам: * <span class="math inline">u_{11} =
\sqrt{a_{11}}</span> * <span class="math inline">u_{1j} = a_{1j} /
u_{11}</span> для <span class="math inline">j = 2, ..., n</span> * <span
class="math inline">u_{ii} = \sqrt{a_{ii} - \sum_{m=1}^{i-1}
u_{mi}^2}</span> для <span class="math inline">i = 2, ..., n</span> *
<span class="math inline">u_{ij} = (a_{ij} - \sum_{m=1}^{i-1}
u_{mi}u_{mj}) / u_{ii}</span> для <span class="math inline">j &gt;
i</span> * <span class="math inline">u_{ij} = 0</span> для <span
class="math inline">j &lt; i</span></p>
<p>Для того чтобы все <span class="math inline">u_{ii}</span> были
действительными, необходимо и достаточно, чтобы матрица <span
class="math inline">A</span> была положительно определенной (все
подкоренные выражения будут положительны).</p>
<p><strong>Решение СЛАУ по схеме Холецкого:</strong></p>
<p>Аналогично общему LU-методу, решение системы <span
class="math inline">Ax=b</span> сводится к двум этапам:</p>
<ol type="1">
<li><strong>Разложение:</strong> Находится матрица <span
class="math inline">V</span> так, что <span class="math inline">A = V^T
V</span>.</li>
<li><strong>Решение:</strong> Система <span class="math inline">V^T Vx =
b</span> решается в два шага:
<ul>
<li><strong>Этап 1: <span class="math inline">V^Tz = b</span></strong>.
Решается система с нижней треугольной матрицей <span
class="math inline">V^T</span> для нахождения вспомогательного вектора
<span class="math inline">z</span> (прямая подстановка).</li>
<li><strong>Этап 2: <span class="math inline">Vx = z</span></strong>.
Решается система с верхней треугольной матрицей <span
class="math inline">V</span> для нахождения искомого вектора <span
class="math inline">x</span> (обратная подстановка).</li>
</ul></li>
</ol>
<p>Этот метод требует примерно вдвое меньше операций, чем обычный метод
Гаусса, и не требует вычисления квадратных корней, если представить
разложение в виде <span class="math inline">A=LDL^T</span>, где <span
class="math inline">D</span> — диагональная.</p>
<hr />
<h3 id="метод-вращения-решения-линейных-систем.">8. Метод вращения
решения линейных систем.</h3>
<p>Метод вращения (метод Гивенса) — это прямой метод решения СЛАУ,
который, в отличие от метода Гаусса, не приводит к росту элементов
матрицы в процессе вычислений, что делает его численно более
устойчивым.</p>
<p><strong>Идея метода:</strong> Метод заключается в последовательном
занулении поддиагональных элементов матрицы путем применения
<strong>ортогональных преобразований (вращений)</strong>. Каждое такое
преобразование затрагивает только две строки системы и “поворачивает” их
в плоскости так, чтобы один из элементов стал равен нулю.</p>
<p><strong>Алгоритм (Прямой ход):</strong></p>
<ol type="1">
<li><strong>Зануление первого столбца:</strong>
<ul>
<li>Чтобы занулить элемент <span class="math inline">a_{21}</span>,
рассматриваются первые две строки. Они умножаются на <span
class="math inline">c_1</span> и <span class="math inline">s_1</span>
(косинус и синус угла вращения) и складываются.</li>
<li>Новые строки 1’ и 2’ формируются так: <span
class="math inline">1&#39; = c_1 \cdot (\text{строка 1}) + s_1 \cdot
(\text{строка 2})</span> <span class="math inline">2&#39; = -s_1 \cdot
(\text{строка 1}) + c_1 \cdot (\text{строка 2})</span></li>
<li>Коэффициенты <span class="math inline">c_1</span> и <span
class="math inline">s_1</span> выбираются из двух условий:
<ol type="1">
<li><span class="math inline">c_1^2 + s_1^2 = 1</span> (условие
ортогональности преобразования).</li>
<li>Коэффициент при <span class="math inline">x_1</span> в новой второй
строке должен стать нулем: <span class="math inline">-s_1 a_{11} + c_1
a_{21} = 0</span>.</li>
</ol></li>
<li>Из этих условий находятся: <span class="math inline">c_1 =
\frac{a_{11}}{\sqrt{a_{11}^2+a_{21}^2}}</span>, <span
class="math inline">s_1 =
\frac{a_{21}}{\sqrt{a_{11}^2+a_{21}^2}}</span>.</li>
<li>После этого преобразования <span class="math inline">a_{21}</span>
обнуляется. Процесс повторяется для зануления <span
class="math inline">a_{31}</span> (вращением строк 1 и 3), <span
class="math inline">a_{41}</span> (вращением 1 и 4) и т.д., пока все
поддиагональные элементы первого столбца не станут нулями.</li>
</ul></li>
<li><strong>Зануление остальных столбцов:</strong>
<ul>
<li>Аналогичная процедура применяется для зануления поддиагональных
элементов второго столбца (вращением строк 2, 3, 4, …), затем третьего и
т.д.</li>
</ul></li>
</ol>
<p>В результате <span class="math inline">n-1</span> этапов прямого хода
матрица приводится к верхнему треугольному виду: <span
class="math display">
\begin{alignedat}{1}
a_{11}^{(n-1)}x_1 + a_{12}^{(n-1)}x_2 + \dots + a_{1n}^{(n-1)}x_n &amp;=
b_1^{(n-1)} \\
a_{22}^{(n-2)}x_2 + \dots + a_{2n}^{(n-2)}x_n &amp;= b_2^{(n-2)} \\
\dots \\
a_{nn}x_n &amp;= b_n^{(n-2)}
\end{alignedat}
</span></p>
<p><strong>Обратный ход:</strong> После приведения матрицы к
треугольному виду, решение находится стандартным обратным ходом, как в
методе Гаусса.</p>
<p><strong>Ключевое свойство:</strong> Преобразование вращения является
ортогональным, оно сохраняет евклидову норму (длину) векторов-столбцов
матрицы. <span class="math inline">(a_{1j}^{(1)})^2 + (a_{2j}^{(1)})^2 =
a_{1j}^2 + a_{2j}^2</span>. Это означает, что элементы матрицы не
растут, что обеспечивает численную устойчивость. Однако метод вращений
требует большего количества арифметических операций (примерно в 2 раза
больше), чем метод Гаусса.</p>
<hr />
<h3
id="итеративные-методы-решения-слау.-принцип-последовательных-приближений.">9.
Итеративные методы решения СЛАУ. Принцип последовательных
приближений.</h3>
<p>В отличие от прямых методов, которые дают точное решение (в
отсутствие ошибок округления) за конечное число шагов,
<strong>итерационные (или итеративные) методы</strong> строят
последовательность приближений <span class="math inline">x^{(0)},
x^{(1)}, ..., x^{(m)}, ...</span>, которая при определенных условиях
сходится к точному решению <span class="math inline">x^*</span>.</p>
<p><strong>Принцип последовательных приближений:</strong></p>
<ol type="1">
<li><p><strong>Преобразование системы:</strong> Исходная система <span
class="math inline">Ax = b</span> преобразуется к эквивалентному виду,
удобному для итераций: <span class="math inline">x = Bx + C</span> Это
можно сделать разными способами, например: <span class="math inline">x =
x + \tau(b - Ax) = (E - \tau A)x + \tau b</span>, где <span
class="math inline">\tau</span> — итерационный параметр. Здесь <span
class="math inline">B = E - \tau A</span>, <span class="math inline">C =
\tau b</span>.</p></li>
<li><p><strong>Построение итерационного процесса:</strong></p>
<ul>
<li>Выбирается <strong>начальное приближение</strong> <span
class="math inline">x^{(0)}</span> (часто нулевой вектор).</li>
<li>Последующие приближения вычисляются по рекуррентной формуле: <span
class="math inline">x^{(m+1)} = Bx^{(m)} + C</span>, где <span
class="math inline">m = 0, 1, 2, ...</span></li>
</ul></li>
<li><p><strong>Условие сходимости:</strong> Процесс сходится к решению
<span class="math inline">x^*</span>, если последовательность <span
class="math inline">x^{(m)}</span> сходится к <span
class="math inline">x^*</span> при <span class="math inline">m \to
\infty</span>. Основным условием сходимости является требование, чтобы
оператор <span class="math inline">B</span> был
<strong>сжимающим</strong>, то есть <span class="math inline">||B|| &lt;
1</span> в некоторой норме.</p></li>
<li><p><strong>Критерий остановки:</strong> Итерации продолжаются до тех
пор, пока не будет достигнута заданная точность. Критерием может
служить:</p>
<ul>
<li>Малость нормы разности двух последовательных приближений: <span
class="math inline">||x^{(m+1)} - x^{(m)}|| &lt;
\varepsilon</span>.</li>
<li>Малость нормы невязки: <span class="math inline">||b - Ax^{(m)}||
&lt; \varepsilon</span>.</li>
</ul></li>
</ol>
<p><strong>Преимущества итерационных методов:</strong> * Особенно
эффективны для больших разреженных систем (с большим количеством нулей),
которые часто возникают при решении дифференциальных уравнений. * Менее
чувствительны к ошибкам округления, так как ошибка на одном шаге может
быть исправлена на последующих. * Проще в программировании.</p>
<p><strong>Недостатки:</strong> * Сходимость не всегда гарантирована. *
Скорость сходимости может быть низкой.</p>
<hr />
<h3 id="простые-итерации-для-решения-слау.">10. Простые итерации для
решения СЛАУ.</h3>
<p>Метод простой итерации (МПИ) — это базовый итерационный метод,
реализующий общий принцип последовательных приближений.</p>
<p><strong>Формы записи МПИ:</strong></p>
<ol type="1">
<li><p><strong>Каноническая форма:</strong> Исходная система <span
class="math inline">Ax = b</span> преобразуется к виду <span
class="math inline">x = Bx + C</span>. Итерационный процесс задается
формулой: <span class="math inline">x^{(m+1)} = Bx^{(m)} + C</span>
Пример преобразования: <span class="math inline">x = (E - \tau A)x +
\tau b</span>, где <span class="math inline">\tau</span> — итерационный
параметр.</p></li>
<li><p><strong>Форма с невязкой:</strong> Вводится понятие
<strong>невязки</strong> <span class="math inline">m</span>-го
приближения: <span class="math inline">r^{(m)} = b - Ax^{(m)}</span>.
Итерационный процесс записывается как: <span
class="math inline">x^{(m+1)} = x^{(m)} + \tau r^{(m)} = x^{(m)} +
\tau(b - Ax^{(m)})</span> Эта форма эквивалентна предыдущей, если <span
class="math inline">B = E - \tau A</span> и <span class="math inline">C
= \tau b</span>. Если приближение <span
class="math inline">x^{(m)}</span> сходится к решению <span
class="math inline">x^*</span>, то невязка <span
class="math inline">r^{(m)}</span> стремится к нулю.</p></li>
</ol>
<p><strong>Выбор начального приближения:</strong> В качестве <span
class="math inline">x^{(0)}</span> обычно берут нулевой вектор <span
class="math inline">(0, 0, ..., 0)^T</span> или вектор правой части
<span class="math inline">b</span>.</p>
<p><strong>Сходимость:</strong> Метод сходится, если выполнены условия,
рассмотренные в следующем вопросе. Скорость сходимости зависит от нормы
матрицы перехода <span class="math inline">B</span>.</p>
<p><strong>Оценка точности:</strong> Зная норму <span
class="math inline">q = ||B||</span>, можно оценить погрешность: *
<strong>Априорная оценка (до начала вычислений):</strong> <span
class="math inline">||x^{(m)} - x^*|| \le \frac{q^m}{1-q}||x^{(1)} -
x^{(0)}||</span> * <strong>Апостериорная оценка (в процессе
вычислений):</strong> <span class="math inline">||x^{(m)} - x^*|| \le
\frac{q}{1-q}||x^{(m)} - x^{(m-1)}||</span> Величину <span
class="math inline">q</span> можно оценить по отношению норм разностей
соседних итераций: <span class="math inline">q \approx \frac{||x^{(m+1)}
- x^{(m)}||}{||x^{(m)} - x^{(m-1)}||}</span></p>
<hr />
<h3 id="достаточное-условие-сходимости-метода-простой-итерации.">11.
Достаточное условие сходимости метода простой итерации.</h3>
<p><strong>Теорема (Достаточное условие сходимости МПИ):</strong>
Итерационный процесс <span class="math inline">x^{(m+1)} = Bx^{(m)} +
C</span> сходится к единственному решению системы <span
class="math inline">x = Bx + C</span> при любом начальном приближении
<span class="math inline">x^{(0)}</span>, если норма матрицы перехода
<span class="math inline">B</span> меньше единицы: <span
class="math inline">||B|| \le q &lt; 1</span> для некоторой матричной
нормы, согласованной с векторной.</p>
<p><strong>Доказательство:</strong> 1. Пусть <span
class="math inline">x^*</span> — точное решение, то есть <span
class="math inline">x^* = Bx^* + C</span>. 2. Рассмотрим погрешность на
<span class="math inline">(m+1)</span>-м шаге: <span
class="math inline">\varepsilon^{(m+1)} = x^{(m+1)} - x^*</span>. <span
class="math inline">\varepsilon^{(m+1)} = (Bx^{(m)} + C) - (Bx^* + C) =
B(x^{(m)} - x^*) = B\varepsilon^{(m)}</span>. 3. Применяя это
соотношение рекурсивно, получаем: <span
class="math inline">\varepsilon^{(m+1)} = B\varepsilon^{(m)} =
B^2\varepsilon^{(m-1)} = \dots = B^{m+1}\varepsilon^{(0)}</span>, где
<span class="math inline">\varepsilon^{(0)} = x^{(0)} - x^*</span>. 4.
Перейдем к нормам: <span class="math inline">||\varepsilon^{(m+1)}|| =
||B^{m+1}\varepsilon^{(0)}|| \le ||B||^{m+1}
||\varepsilon^{(0)}||</span>. 5. По условию теоремы, <span
class="math inline">||B|| \le q &lt; 1</span>. Следовательно: <span
class="math inline">||\varepsilon^{(m+1)}|| \le q^{m+1}
||\varepsilon^{(0)}||</span>. 6. Поскольку <span class="math inline">q
&lt; 1</span>, то <span class="math inline">q^{m+1} \to 0</span> при
<span class="math inline">m \to \infty</span>. Это означает, что <span
class="math inline">||\varepsilon^{(m+1)}|| \to 0</span>, и,
следовательно, <span class="math inline">x^{(m+1)} \to x^*</span>.</p>
<p><strong>Скорость сходимости:</strong> Неравенство <span
class="math inline">||\varepsilon^{(m)}|| \le q^m
||\varepsilon^{(0)}||</span> показывает, что погрешность уменьшается со
скоростью <strong>геометрической прогрессии</strong> со знаменателем
<span class="math inline">q</span>. Чем меньше <span
class="math inline">q</span>, тем выше скорость сходимости.</p>
<p><strong>Необходимое и достаточное условие (без доказательства в
лекциях):</strong> Метод <span class="math inline">x = Bx+C</span>
сходится при любом <span class="math inline">x^{(0)}</span> тогда и
только тогда, когда все собственные числа <span
class="math inline">\lambda</span> матрицы <span
class="math inline">B</span> по модулю меньше единицы: <span
class="math inline">|\lambda_i(B)| &lt; 1</span> для всех <span
class="math inline">i</span>.</p>
<hr />
<h3 id="метод-якоби-решения-слау.">12. Метод Якоби решения СЛАУ.</h3>
<p>Метод Якоби — это один из классических итерационных методов,
являющийся частным случаем метода простой итерации.</p>
<p><strong>Идея метода:</strong> В <span class="math inline">i</span>-м
уравнении системы <span class="math inline">Ax=b</span> выражается <span
class="math inline">i</span>-я компонента вектора <span
class="math inline">x</span> через все остальные. Для построения
следующего приближения <span class="math inline">x^{(m+1)}</span>
используются только компоненты предыдущего приближения <span
class="math inline">x^{(m)}</span>.</p>
<p><strong>Вывод формулы:</strong> 1. Представим матрицу <span
class="math inline">A</span> в виде суммы трех матриц: <span
class="math inline">A = L + D + U</span>, где * <span
class="math inline">L = \begin{pmatrix} 0 &amp; &amp; \\ a_{21} &amp; 0
&amp; \\ \vdots &amp; \ddots &amp; \\ a_{n1} &amp; \dots &amp; 0
\end{pmatrix}</span> — строго нижнетреугольная часть <span
class="math inline">A</span>. * <span class="math inline">D =
\begin{pmatrix} a_{11} &amp; &amp; 0 \\ &amp; \ddots &amp; \\ 0 &amp;
&amp; a_{nn} \end{pmatrix}</span> — диагональная часть <span
class="math inline">A</span>. * <span class="math inline">U =
\begin{pmatrix} 0 &amp; a_{12} &amp; \dots &amp; a_{1n} \\ &amp; \ddots
&amp; \vdots \\ &amp; &amp; 0 \end{pmatrix}</span> — строго
верхнетреугольная часть <span class="math inline">A</span>. 2. Исходная
система <span class="math inline">Ax=b</span> записывается как <span
class="math inline">(L+D+U)x = b</span>. 3. Для итерационного процесса
<span class="math inline">(m+1)</span>-е приближение <span
class="math inline">x^{(m+1)}</span> связывается с <span
class="math inline">D</span>, а <span class="math inline">m</span>-е — с
<span class="math inline">L</span> и <span class="math inline">U</span>:
<span class="math inline">Dx^{(m+1)} + (L+U)x^{(m)} = b</span> 4. Отсюда
выражается <span class="math inline">x^{(m+1)}</span>: <span
class="math inline">x^{(m+1)} = -D^{-1}(L+U)x^{(m)} + D^{-1}b</span> Это
и есть формула метода Якоби в матричной форме. Матрица перехода здесь
<span class="math inline">B_J = -D^{-1}(L+U)</span>.</p>
<p><strong>Покомпонентная запись:</strong> Из <span
class="math inline">i</span>-го уравнения <span
class="math inline">\sum_{j=1}^{n} a_{ij}x_j = b_i</span> выражаем <span
class="math inline">x_i</span>: <span class="math inline">a_{ii}x_i =
b_i - \sum_{j \ne i} a_{ij}x_j</span>. Итерационная формула для <span
class="math inline">i</span>-й компоненты: <span class="math display">
\left\{
\begin{alignedat}{1}
x_1^{(m+1)} &amp;= -(a_{12}x_2^{(m)} + a_{13}x_3^{(m)} + \dots +
a_{1n}x_n^{(m)} - b_1)/a_{11} \\
&amp;\dots \\
x_n^{(m+1)} &amp;= -(a_{n1}x_1^{(m)} + a_{n2}x_2^{(m)} + \dots +
a_{n,n-1}x_{n-1}^{(m)} - b_n)/a_{nn}
\end{alignedat}
\right.
</span> или <span class="math inline">x_i^{(m+1)} = \frac{1}{a_{ii}}
\left( b_i - \sum_{\substack{j=1 \\ j \ne i}}^n a_{ij}x_j^{(m)}
\right)</span>, где <span class="math inline">i=1, ..., n</span>.</p>
<p><strong>Особенности:</strong> * Все компоненты вектора <span
class="math inline">x^{(m+1)}</span> могут вычисляться одновременно
(параллельно), так как для расчета <span
class="math inline">x_i^{(m+1)}</span> нужны только значения из <span
class="math inline">x^{(m)}</span>. * Требуется хранить в памяти два
вектора: <span class="math inline">x^{(m)}</span> и <span
class="math inline">x^{(m+1)}</span>.</p>
<hr />
<h3
id="диагональное-преобладание-и-достаточное-условие-сходимости-метода-якоби.-критерий-сходимости-метода-якоби.">13.
Диагональное преобладание и достаточное условие сходимости метода Якоби.
Критерий сходимости метода Якоби.</h3>
<p><strong>Диагональное преобладание:</strong> Говорят, что матрица
<span class="math inline">A</span> обладает <strong>строгим диагональным
преобладанием</strong>, если для каждой строки модуль диагонального
элемента больше суммы модулей всех остальных элементов в этой строке:
<span class="math inline">|a_{ii}| &gt; \sum_{\substack{j=1 \\ j \ne
i}}^n |a_{ij}|</span>, для всех <span class="math inline">i=1, ...,
n</span>.</p>
<p><strong>Теорема (Достаточное условие сходимости метода
Якоби):</strong> Если матрица <span class="math inline">A</span> системы
<span class="math inline">Ax=b</span> обладает строгим диагональным
преобладанием, то метод Якоби сходится к единственному решению системы
при любом начальном приближении.</p>
<p><strong>Обоснование:</strong> Это условие гарантирует, что норма
матрицы перехода <span class="math inline">B_J = -D^{-1}(L+U)</span>
будет меньше 1. Рассмотрим <span class="math inline">\infty</span>-норму
(максимальная сумма модулей по строкам): <span
class="math inline">||B_J||_\infty = \max_i \sum_{\substack{j=1 \\ j \ne
i}}^n \left|-\frac{a_{ij}}{a_{ii}}\right| = \max_i \frac{1}{|a_{ii}|}
\sum_{\substack{j=1 \\ j \ne i}}^n |a_{ij}|</span>. Из условия
диагонального преобладания следует, что <span
class="math inline">\frac{1}{|a_{ii}|} \sum_{\substack{j=1 \\ j \ne
i}}^n |a_{ij}| &lt; 1</span> для каждой строки <span
class="math inline">i</span>. Следовательно, <span
class="math inline">||B_J||_\infty &lt; 1</span>, и по общей теореме о
сходимости МПИ, метод Якоби сходится.</p>
<p><strong>Теорема (Критерий сходимости метода Якоби):</strong> Для
сходимости итераций метода Якоби необходимо и достаточно, чтобы все
корни <span class="math inline">\lambda</span> характеристического
уравнения <span class="math inline">det(B_J - \lambda E) = 0</span> были
по модулю меньше единицы. Это уравнение можно преобразовать к
эквивалентному виду, более удобному для анализа: <span
class="math inline">det(\lambda D + L + U) = 0</span> или в развернутой
форме: <span class="math display">
\begin{vmatrix}
\lambda a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
a_{21} &amp; \lambda a_{22} &amp; \dots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; a_{n2} &amp; \dots &amp; \lambda a_{nn}
\end{vmatrix} = 0
</span> Все корни <span class="math inline">\lambda</span> этого
уравнения должны быть по модулю меньше 1.</p>
<hr />
<h3 id="метод-зейделя-решения-слау.">14. Метод Зейделя решения
СЛАУ.</h3>
<p>Метод Зейделя (или метод Гаусса-Зейделя) — это модификация метода
Якоби, которая часто обеспечивает более быструю сходимость.</p>
<p><strong>Идея метода:</strong> Основное отличие от метода Якоби
состоит в том, что при вычислении <span
class="math inline">(m+1)</span>-го приближения для компоненты <span
class="math inline">x_i^{(m+1)}</span> используются уже вычисленные на
этом же шаге компоненты <span class="math inline">x_1^{(m+1)}, ...,
x_{i-1}^{(m+1)}</span>.</p>
<p><strong>Вывод формулы:</strong> 1. Снова представим <span
class="math inline">A = L+D+U</span>. Система: <span
class="math inline">(L+D+U)x = b</span>. 2. Итерационный процесс
строится так, чтобы уже вычисленные на <span
class="math inline">(m+1)</span>-м шаге компоненты (связанные с <span
class="math inline">L</span>) и диагональные (связанные с <span
class="math inline">D</span>) относились к <span
class="math inline">x^{(m+1)}</span>, а еще не обновленные (связанные с
<span class="math inline">U</span>) — к <span
class="math inline">x^{(m)}</span>: <span
class="math inline">(L+D)x^{(m+1)} + Ux^{(m)} = b</span> 3. Отсюда
выражается <span class="math inline">x^{(m+1)}</span>: <span
class="math inline">x^{(m+1)} = -(L+D)^{-1}Ux^{(m)} + (L+D)^{-1}b</span>
Это формула метода Зейделя в матричной форме. Матрица перехода: <span
class="math inline">B_S = -(L+D)^{-1}U</span>.</p>
<p><strong>Покомпонентная запись:</strong> Вычисления производятся
последовательно для <span class="math inline">i = 1, ..., n</span>:
<span class="math inline">a_{11}x_1^{(m+1)} = -a_{12}x_2^{(m)} -
a_{13}x_3^{(m)} - \dots - a_{1n}x_n^{(m)} + b_1</span> <span
class="math inline">a_{22}x_2^{(m+1)} = -a_{21}x_1^{(m+1)} -
a_{23}x_3^{(m)} - \dots - a_{2n}x_n^{(m)} + b_2</span> <span
class="math inline">\dots</span> <span
class="math inline">a_{nn}x_n^{(m+1)} = -a_{n1}x_1^{(m+1)} -
a_{n2}x_2^{(m+1)} - \dots - a_{n,n-1}x_{n-1}^{(m+1)} + b_n</span> или
<span class="math inline">x_i^{(m+1)} = \frac{1}{a_{ii}} \left( b_i -
\sum_{j=1}^{i-1} a_{ij}x_j^{(m+1)} - \sum_{j=i+1}^{n} a_{ij}x_j^{(m)}
\right)</span></p>
<p><strong>Особенности:</strong> * Компоненты <span
class="math inline">x^{(m+1)}</span> нельзя вычислять параллельно, так
как расчет <span class="math inline">x_i^{(m+1)}</span> зависит от <span
class="math inline">x_1^{(m+1)}, ..., x_{i-1}^{(m+1)}</span>. * Обычно
требуется хранить только один вектор <span class="math inline">x</span>,
значения которого последовательно обновляются. * Часто сходится быстрее
метода Якоби (примерно в 2 раза для некоторых классов задач).</p>
<hr />
<h3
id="достаточное-условие-сходимости-метода-зейделя-для-решения-слау.-критерий-сходимости-метода-зейделя.">15.
Достаточное условие сходимости метода Зейделя для решения СЛАУ. Критерий
сходимости метода Зейделя.</h3>
<p><strong>Достаточные условия сходимости:</strong> 1. Как и для метода
Якоби, <strong>строгое диагональное преобладание</strong> матрицы <span
class="math inline">A</span> является достаточным условием сходимости
метода Зейделя. 2. <strong>Теорема:</strong> Пусть <span
class="math inline">A</span> — вещественная, симметричная и положительно
определенная матрица. Тогда метод Зейделя сходится при любом начальном
приближении.</p>
<p><strong>Теорема (Критерий сходимости метода Зейделя):</strong> Для
сходимости итерационного метода Зейделя необходимо и достаточно, чтобы
все корни <span class="math inline">\lambda</span> характеристического
уравнения <span class="math inline">det(B_S - \lambda E) = 0</span> были
по модулю меньше единицы, где <span class="math inline">B_S =
-(L+D)^{-1}U</span> — матрица перехода. Это условие эквивалентно
требованию, чтобы все корни уравнения <span class="math inline">det(U +
\lambda(L+D)) = 0</span> были по модулю меньше единицы. В развернутой
форме это уравнение выглядит так: <span class="math display">
\begin{vmatrix}
\lambda a_{11} &amp; a_{12} &amp; \dots &amp; a_{1n} \\
\lambda a_{21} &amp; \lambda a_{22} &amp; \dots &amp; \lambda a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\lambda a_{n1} &amp; \lambda a_{n2} &amp; \dots &amp; \lambda a_{nn}
\end{vmatrix} = 0
</span> Здесь в определителе <span class="math inline">i</span>-я строка
умножается на <span class="math inline">\lambda</span> только для
элементов с <span class="math inline">j \ge i</span>.</p>
<hr />
<h3
id="проблема-выбора-итерационного-параметра.-связь-между-слау-и-разностными-методами-для-дифференциальных-уравнений.">16.
Проблема выбора итерационного параметра. Связь между СЛАУ и разностными
методами для дифференциальных уравнений.</h3>
<p><strong>Проблема выбора итерационного параметра (<span
class="math inline">\tau</span>):</strong></p>
<p>В методах типа простой итерации (<span class="math inline">x^{(m+1)}
= x^{(m)} + \tau r^{(m)}</span>) и методе верхней релаксации, скорость
сходимости и даже сам факт сходимости критически зависят от выбора
<strong>итерационного параметра</strong> <span
class="math inline">\tau</span>.</p>
<ul>
<li><strong>Сходимость:</strong> Для МПИ с симметричной положительно
определенной матрицей <span class="math inline">A</span> процесс
сходится, если <span class="math inline">0 &lt; \tau &lt;
\frac{2}{\lambda_{\max}(A)}</span>. Если <span
class="math inline">\tau</span> выбрать за пределами этого интервала,
итерации будут расходиться.</li>
<li><strong>Скорость сходимости:</strong> Внутри интервала сходимости
существует <strong>оптимальное значение</strong> <span
class="math inline">\tau_{\text{опт}}</span>, при котором скорость
сходимости максимальна.</li>
</ul>
<p>Задача состоит в том, чтобы найти это оптимальное значение <span
class="math inline">\tau_{\text{опт}}</span>, что часто требует знания
границ спектра (минимального <span
class="math inline">\lambda_{\min}</span> и максимального <span
class="math inline">\lambda_{\max}</span> собственных значений) матрицы
<span class="math inline">A</span>, которые, в свою очередь, найти
непросто.</p>
<p><strong>Метод верхней релаксации (SOR - Successive
Over-Relaxation):</strong> Это развитие метода Зейделя, в которое
вводится параметр релаксации <span class="math inline">\tau</span>:
<span class="math inline">(\tau L) x^{(m+1)} + D x^{(m+1)} + (1-\tau)D
x^{(m)} + \tau U x^{(m)} = \tau b</span>. Выразив <span
class="math inline">x^{(m+1)}</span>: <span
class="math inline">x^{(m+1)} = -(D+\tau L)^{-1}((1-\tau)D+\tau
U)x^{(m)} + \tau(D+\tau L)^{-1}b</span>. Выбор <span
class="math inline">\tau</span> позволяет управлять скоростью
сходимости. * <span class="math inline">0 &lt; \tau &lt; 1</span>:
нижняя релаксация. * <span class="math inline">\tau = 1</span>: метод
Зейделя. * <span class="math inline">1 &lt; \tau &lt; 2</span>: верхняя
релаксация (часто дает ускорение). В общем случае задача поиска <span
class="math inline">\tau_{\text{опт}}</span> не решена, но известно, что
<span class="math inline">1 &lt; \tau_{\text{опт}} &lt; 2</span>.</p>
<p><strong>Связь СЛАУ и разностных методов:</strong></p>
<p>Многие задачи математической физики, описываемые дифференциальными
уравнениями (например, уравнение теплопроводности, уравнение Пуассона),
для численного решения дискретизируются. Это означает, что область
решения покрывается сеткой, а производные заменяются <strong>конечными
разностями</strong>.</p>
<p>Этот процесс, называемый <strong>сеточной или разностной
аппроксимацией</strong>, приводит к замене одного дифференциального
уравнения на систему алгебраических уравнений, где неизвестными являются
значения искомой функции в узлах сетки.</p>
<ul>
<li>Часто эти системы являются СЛАУ.</li>
<li>Матрицы таких СЛАУ обычно <strong>большие</strong> (тысячи и
миллионы уравнений), <strong>разреженные</strong> (много нулей) и имеют
<strong>ленточную структуру</strong> (например, трехдиагональную).</li>
<li>Именно для таких систем итерационные методы особенно
эффективны.</li>
</ul>
<p>Таким образом, решение дифференциальных уравнений численно часто
сводится к решению СЛАУ, а свойства этой СЛАУ (например, спектр матрицы)
напрямую связаны со свойствами исходной дифференциальной задачи.</p>
<hr />
<h3
id="оптимизация-скорости-сходимости-итерационных-процессов.-выбор-оптимального-итеративного-параметра.-чебышевское-ускорение-итераций.">17.
Оптимизация скорости сходимости итерационных процессов. Выбор
оптимального итеративного параметра. Чебышевское ускорение
итераций.</h3>
<p>Для метода простой итерации <span class="math inline">x^{(m+1)} = (E
- \tau A)x^{(m)} + \tau b</span> с симметричной положительно
определенной матрицей <span class="math inline">A</span>, скорость
сходимости определяется нормой оператора перехода <span
class="math inline">q = ||E - \tau A||</span>. Для минимизации <span
class="math inline">q</span> (и ускорения сходимости) необходимо решить
задачу: <span class="math inline">\min_\tau \max_\lambda |1 -
\tau\lambda|</span>, где <span class="math inline">\lambda</span>
пробегает все собственные значения матрицы <span
class="math inline">A</span>.</p>
<p><strong>1. Выбор оптимального постоянного параметра <span
class="math inline">\tau_0</span>:</strong></p>
<ul>
<li>Пусть собственные значения <span class="math inline">A</span> лежат
в интервале <span class="math inline">[l, L]</span>, где <span
class="math inline">l = \lambda_{\min} &gt; 0</span>, <span
class="math inline">L = \lambda_{\max} &gt; 0</span>.</li>
<li>Функция <span class="math inline">y(\tau) = |1 - \tau\lambda|</span>
максимальна на концах интервала, то есть <span
class="math inline">\max_\lambda|1-\tau\lambda| = \max(|1-\tau l|,
|1-\tau L|)</span>.</li>
<li>Минимум этой величины достигается, когда <span
class="math inline">|1-\tau_0 l| = |1-\tau_0 L|</span>. Отсюда находится
<strong>оптимальный параметр <span
class="math inline">\tau_0</span></strong>: <span
class="math inline">\tau_0 = \frac{2}{l+L}</span>.</li>
<li>При таком <span class="math inline">\tau_0</span> скорость
сходимости (знаменатель прогрессии) равна: <span class="math inline">q =
\max_{\lambda \in [l,L]} |1-\tau_0\lambda| = 1 - \frac{2}{L/l+1} =
\frac{L/l-1}{L/l+1}</span>. При <span class="math inline">l/L \ll
1</span>, <span class="math inline">q \approx 1 -
2\frac{l}{L}</span>.</li>
<li>Число итераций <span class="math inline">m</span> для достижения
точности <span class="math inline">\varepsilon</span> оценивается как
<span class="math inline">m_{\tau_0} \approx
\frac{\ln\varepsilon}{\ln(1-2l/L)} \approx
\frac{L}{2l}\ln\frac{1}{\varepsilon}</span>.</li>
</ul>
<p><strong>2. Чебышевское ускорение итераций (метод с переменным
параметром):</strong></p>
<p>Идея состоит в использовании на каждом шаге своего параметра <span
class="math inline">\tau_j</span>, чтобы минимизировать ошибку за <span
class="math inline">m</span> шагов. * Ошибка после <span
class="math inline">m</span> шагов: <span
class="math inline">\delta^{(m)} = \prod_{j=1}^m (E - \tau_j A)
\delta^{(0)}</span>. * Задача сводится к: <span
class="math inline">\min_{\{\tau_j\}} \max_{\lambda \in [l,L]}
\left|\prod_{j=1}^m (1 - \tau_j\lambda)\right|</span>. * Решением этой
задачи является <strong>полином Чебышева</strong>, наименее уклоняющийся
от нуля на отрезке. Корни этого полинома определяют оптимальный набор
итерационных параметров <span class="math inline">\{\tau_j\}</span>:
<span class="math inline">\tau_j = \left[\frac{L+l}{2} + \frac{L-l}{2}
\cos \frac{\pi(2j-1)}{2m}\right]^{-1}, \quad j=1,...,m</span>. * Для
такого набора параметров скорость сходимости значительно выше: <span
class="math inline">q \approx 1-2\sqrt{\frac{l}{L}}</span>. * Число
итераций оценивается как <span class="math inline">m_ч \approx
\frac{\ln\varepsilon}{\ln(1-2\sqrt{l/L})} \approx
\frac{1}{2}\sqrt{\frac{L}{l}}\ln\frac{1}{\varepsilon}</span>. *
<strong>Эффективность:</strong> <span
class="math inline">\frac{m_{\tau_0}}{m_ч} \approx
\sqrt{\frac{L}{l}}</span>. Если <span class="math inline">L/l</span>
велико (плохо обусловленная матрица), ускорение может быть очень
существенным.</p>
<p><strong>Недостаток:</strong> Чебышевский набор параметров в
естественном порядке приводит к численно неустойчивому процессу. Для
устойчивости параметры <span class="math inline">\{\tau_j\}</span> нужно
использовать в специальном, “перемешанном” порядке.</p>
<hr />
<h3
id="подходы-к-решению-нелинейных-уравнений.-вопрос-выбора-начального-приближения.-отделение-корня-интервал-изоляции.">18.
Подходы к решению нелинейных уравнений. Вопрос выбора начального
приближения. Отделение корня, интервал изоляции.</h3>
<p>Задача решения нелинейного уравнения состоит в нахождении корней
(нулей) <span class="math inline">x^*</span> уравнения <span
class="math inline">f(x) = 0</span>.</p>
<p><strong>Этапы решения:</strong> 1. <strong>Локализация (отделение)
корней:</strong> Определение интервалов, в каждом из которых содержится
ровно один корень. Такой интервал называется <strong>интервалом
изоляции</strong>. 2. <strong>Уточнение корня:</strong> Нахождение
значения корня в интервале изоляции с заданной точностью.</p>
<p><strong>Методы локализации корней:</strong> *
<strong>Графический:</strong> Построение графика функции <span
class="math inline">y=f(x)</span> и нахождение точек его пересечения с
осью <span class="math inline">Ox</span>. *
<strong>Аналитический:</strong> Использование свойств функции
(непрерывность, монотонность, знаки на концах отрезка). Если непрерывная
функция <span class="math inline">f(x)</span> принимает на концах
отрезка <span class="math inline">[a, b]</span> значения разных знаков
(<span class="math inline">f(a)f(b) &lt; 0</span>), то на этом отрезке
есть как минимум один корень. Если к тому же производная <span
class="math inline">f&#39;(x)</span> сохраняет знак на <span
class="math inline">[a, b]</span>, то корень на этом отрезке
единственный. * <strong>Табулирование:</strong> Вычисление значений
функции в ряде точек и поиск смены знака.</p>
<p><strong>Вопрос выбора начального приближения:</strong> Выбор
начального приближения <span class="math inline">x_0</span> критически
важен для сходимости и скорости сходимости итерационных методов. * Для
<strong>двухточечных методов</strong> (дихотомия, хорды) требуется
задать интервал изоляции <span class="math inline">[a, b]</span>. * Для
<strong>одноточечных методов</strong> (Ньютона, МПИ) требуется задать
одно начальное приближение <span class="math inline">x_0</span>. Хороший
выбор <span class="math inline">x_0</span> (достаточно близко к корню)
обеспечивает сходимость. Плохой выбор может привести к расходимости,
зацикливанию или сходимости к другому корню.</p>
<p><strong>Классификация методов уточнения корней:</strong> *
<strong>Двухточечные:</strong> Используют информацию об интервале
локализации. Гарантируют сходимость, но могут быть медленными. (Пример:
дихотомия, хорды). * <strong>Одноточечные:</strong> Не требуют
интервала, используют значение функции и её производных в одной точке.
Могут быть очень быстрыми, но сходимость не гарантирована. (Пример:
метод Ньютона).</p>
<hr />
<h3 id="метод-дихотомии-и-метод-хорд-решения-нелинейного-уравнения.">19.
Метод дихотомии и метод хорд решения нелинейного уравнения.</h3>
<p>Это двухточечные методы, требующие задания интервала изоляции <span
class="math inline">[a, b]</span>, на концах которого <span
class="math inline">f(a)f(b) &lt; 0</span>.</p>
<p><strong>1. Метод деления отрезка пополам (дихотомия):</strong></p>
<ul>
<li><strong>Идея:</strong> На каждом шаге интервал, содержащий корень,
делится пополам. Для следующего шага выбирается та половина, на концах
которой функция сохраняет разные знаки.</li>
<li><strong>Алгоритм:</strong>
<ol type="1">
<li>Задан интервал <span class="math inline">[a_0, b_0]</span>.</li>
<li>На <span class="math inline">n</span>-м шаге вычисляется середина
отрезка <span class="math inline">c_n = (a_n + b_n)/2</span>.</li>
<li>Вычисляется <span class="math inline">f(c_n)</span>.</li>
<li>Если <span class="math inline">f(a_n)f(c_n) &lt; 0</span>, то новый
отрезок <span class="math inline">[a_{n+1}, b_{n+1}] = [a_n,
c_n]</span>.</li>
<li>Иначе (если <span class="math inline">f(c_n)f(b_n) &lt; 0</span>),
новый отрезок <span class="math inline">[a_{n+1}, b_{n+1}] = [c_n,
b_n]</span>.</li>
<li>Процесс повторяется до тех пор, пока длина отрезка <span
class="math inline">(b_n - a_n)</span> не станет меньше заданной
точности <span class="math inline">2\varepsilon</span>.</li>
</ol></li>
<li><strong>Свойства:</strong>
<ul>
<li><strong>Надежность:</strong> Метод всегда сходится, если <span
class="math inline">f(x)</span> непрерывна.</li>
<li><strong>Скорость:</strong> <strong>Линейная</strong>. Длина отрезка
на <span class="math inline">n</span>-м шаге <span
class="math inline">(b-a)/2^n</span>. <span
class="math inline">|x_n-x^*| \le \frac{1}{2}(b-a)2^{-n}</span>.</li>
<li><strong>Недостаток:</strong> Не учитывает значения функции, только
их знаки. Не находит корни четной кратности.</li>
</ul></li>
</ul>
<p><strong>2. Метод хорд (метод секущих):</strong></p>
<ul>
<li><strong>Идея:</strong> Вместо деления отрезка пополам, он делится в
точке пересечения <strong>хорды</strong> (секущей), соединяющей точки
<span class="math inline">(a, f(a))</span> и <span
class="math inline">(b, f(b))</span>, с осью <span
class="math inline">Ox</span>.</li>
<li><strong>Алгоритм (с проверкой знаков):</strong>
<ol type="1">
<li>Уравнение хорды: <span class="math inline">\frac{y - f(a)}{x-a} =
\frac{f(b)-f(a)}{b-a}</span>.</li>
<li>Точка пересечения с осью <span class="math inline">y=0</span>: <span
class="math inline">c = a - \frac{f(a)(b-a)}{f(b)-f(a)}</span>.</li>
<li>Как и в дихотомии, выбирается новый подынтервал (<span
class="math inline">[a, c]</span> или <span class="math inline">[c,
b]</span>), на концах которого <span class="math inline">f(x)</span>
имеет разные знаки.</li>
</ol></li>
<li><strong>Вариант без проверки знаков (метод секущих):</strong>
Итерационная формула, использующая две последние точки <span
class="math inline">x_n</span> и <span
class="math inline">x_{n-1}</span>: <span class="math inline">x_{n+1} =
x_n - \frac{f(x_n)}{ (f(x_n) - f(x_{n-1}))/(x_n-x_{n-1}) }</span></li>
<li><strong>Свойства:</strong>
<ul>
<li>Скорость сходимости обычно выше, чем у дихотомии (сверхлинейная,
порядок <span class="math inline">\approx 1.618</span>), особенно если
функция близка к линейной.</li>
<li>Менее надежен, чем дихотомия (вариант без проверки знаков может
разойтись).</li>
</ul></li>
</ul>
<hr />
<h3 id="метод-ньютона.">20. Метод Ньютона.</h3>
<p>Метод Ньютона (метод касательных) — это один из самых эффективных
одноточечных методов решения нелинейного уравнения <span
class="math inline">f(x) = 0</span>.</p>
<p><strong>Идея метода:</strong> На каждом шаге итерации кривая <span
class="math inline">y=f(x)</span> заменяется
<strong>касательной</strong>, проведенной в текущей точке приближения
<span class="math inline">(x_n, f(x_n))</span>. Следующее приближение
<span class="math inline">x_{n+1}</span> — это точка пересечения этой
касательной с осью <span class="math inline">Ox</span>.</p>
<p><strong>Вывод формулы:</strong> 1. Будем использовать приближенную
формулу <span class="math inline">f(x) \approx f(x_n) +
f&#39;(x_n)(x-x_n)</span>. 2. Вместо исходного уравнения <span
class="math inline">f(x)=0</span> воспользуемся линейным уравнением:
<span class="math inline">f(x_n) + f&#39;(x_n)(x - x_n) = 0</span>. 3.
Решение этого уравнения примем за приближение <span
class="math inline">x_{n+1}</span>: <span class="math inline">x_{n+1} =
x_n - \frac{f(x_n)}{f&#39;(x_n)}</span>, для <span
class="math inline">n=0,1,2,3,...</span></p>
<p><strong>Условия сходимости:</strong> 1.
<strong>Геометрические:</strong> Метод сходится, если начальное
приближение <span class="math inline">x_0</span> выбрано достаточно
близко к корню <span class="math inline">x^*</span>, а функция <span
class="math inline">f(x)</span> “ведет себя хорошо” (не слишком пологая,
без резких изгибов). 2. <strong>Аналитические:</strong> Пусть всюду на
<span class="math inline">[a,b]</span> выполняются условия: <span
class="math inline">|f&#39;(x)| &gt; M_1 &gt; 0</span> <span
class="math inline">|f&#39;&#39;(x)| &lt; M_2</span> Тогда, если нулевое
приближение <span class="math inline">x_0</span> находится в <span
class="math inline">\varepsilon</span>-окрестности корня <span
class="math inline">x^*</span> (<span
class="math inline">O_\varepsilon(x_*)</span>), то итерационный процесс
сходится. Более строго, сходимость гарантирована, если на интервале
изоляции <span class="math inline">[a, b]</span> производные <span
class="math inline">f&#39;(x)</span> и <span
class="math inline">f&#39;&#39;(x)</span> сохраняют знак, и <span
class="math inline">x_0</span> выбрано так, что <span
class="math inline">f(x_0)f&#39;&#39;(x_0) &gt; 0</span>.</p>
<p><strong>Скорость сходимости:</strong> *
<strong>Квадратичная:</strong> Если <span
class="math inline">f&#39;(x^*) \ne 0</span> (простой корень), то <span
class="math inline">|x_{n+1}-x^*| \le
\frac{M_2}{2M_1}|x_n-x^*|^2</span>. Это означает, что на каждом шаге
количество верных значащих цифр примерно удваивается. *
<strong>Линейная:</strong> Если корень кратный (<span
class="math inline">f&#39;(x^*)=0</span>), сходимость замедляется до
линейной.</p>
<p><strong>Недостатки:</strong> * Требует вычисления производной на
каждом шаге, что может быть трудоемко. * Локальный характер сходимости
(требует хорошего <span class="math inline">x_0</span>).</p>
<hr />
<h3 id="применение-метода-ньютона-к-вычислению-значений-функций.">21.
Применение метода Ньютона к вычислению значений функций.</h3>
<p>Метод Ньютона можно использовать для вычисления значений различных
функций (<span class="math inline">y=f(a)</span>), особенно если прямые
вычисления сложны, а операция деления доступна.</p>
<p><strong>Общий подход:</strong> 1. <strong>Неявное
представление:</strong> Требуемое значение <span
class="math inline">x=f(a)</span> представляется как корень неявного
уравнения <span class="math inline">F(a, x) = 0</span>. Функция <span
class="math inline">F</span> подбирается так, чтобы она была легко
дифференцируема по <span class="math inline">x</span>, а ее вычисление
было простым. 2. <strong>Применение метода Ньютона:</strong> К уравнению
<span class="math inline">F(a, x) = 0</span> (где <span
class="math inline">a</span> — параметр) применяется итерационная
формула Ньютона для нахождения <span class="math inline">x</span>: <span
class="math inline">x_{n+1} = x_n - \frac{F(a, x_n)}{F&#39;_x(a,
x_n)}</span>, для <span class="math inline">n=0,1,2,3...</span></p>
<p><strong>Примеры:</strong></p>
<ol type="1">
<li><strong>Вычисление квадратного корня <span class="math inline">x =
\sqrt{a}</span>:</strong>
<ul>
<li>Уравнение: <span class="math inline">x^2 - a = 0</span>.</li>
<li><span class="math inline">F(a, x) = x^2 - a</span>, <span
class="math inline">F&#39;_x(a, x) = 2x</span>.</li>
<li>Итерационная формула (формула Герона): <span
class="math inline">x_{n+1} = x_n - \frac{x_n^2 - a}{2x_n} =
\frac{1}{2}\left(x_n+\frac{a}{x_n}\right)</span></li>
</ul></li>
<li><strong>Вычисление корня m-й степени <span class="math inline">x =
\sqrt[m]{a}</span>:</strong>
<ul>
<li>Уравнение: <span class="math inline">x^m - a = 0</span>.</li>
<li><span class="math inline">F(a, x) = x^m - a</span>, <span
class="math inline">F&#39;_x(a, x) = mx^{m-1}</span>.</li>
<li>Итерационная формула: <span class="math inline">x_{n+1} = x_n -
\frac{x_n^m - a}{mx_n^{m-1}} = \frac{1}{m}((m-1)x_n +
\frac{a}{x_n^{m-1}})</span></li>
</ul></li>
<li><strong>Вычисление обратной величины <span class="math inline">x =
1/a</span> без деления:</strong>
<ul>
<li>Уравнение: <span class="math inline">a - \frac{1}{x} =
0</span>.</li>
<li><span class="math inline">F(a, x) = a - \frac{1}{x}</span>, <span
class="math inline">F&#39;_x(a, x) = \frac{1}{x^2}</span>.</li>
<li>Итерационная формула: <span class="math inline">x_{n+1} = x_n -
\frac{a-1/x_n}{1/x_n^2} = x_n(2-ax_n)</span> Этот процесс использует
только умножение и вычитание. Итерации сходятся, если <span
class="math inline">x_0 \in (0, 2/a)</span>.</li>
</ul></li>
</ol>
<hr />
<h3 id="модификации-метода-ньютона.-метод-ньютона-шрёдера.">22.
Модификации метода Ньютона. Метод Ньютона-Шрёдера.</h3>
<p>Основной недостаток метода Ньютона — необходимость вычислять
производную <span class="math inline">f&#39;(x_n)</span> на каждой
итерации. Модификации направлены на упрощение этого шага.</p>
<p><strong>1. Упрощенный (огрубленный) метод Ньютона:</strong></p>
<ul>
<li><strong>Идея:</strong> Производная вычисляется только один раз в
начальной точке <span class="math inline">x_0</span> и используется на
всех последующих шагах.</li>
<li><strong>Формула:</strong> <span class="math inline">x_{n+1} = x_n -
\frac{f(x_n)}{f&#39;(x_0)}</span>, для <span
class="math inline">n=0,1,2,3...</span></li>
<li><strong>Геометрический смысл:</strong> Все последующие секущие
параллельны касательной в точке <span
class="math inline">x_0</span>.</li>
<li><strong>Скорость:</strong> Квадратичная сходимость теряется, метод
сходится <strong>линейно</strong>.</li>
</ul>
<p><strong>2. Разностный метод Ньютона (метод секущих):</strong></p>
<ul>
<li><strong>Идея:</strong> Производная <span
class="math inline">f&#39;(x_n)</span> аппроксимируется конечной
разностью.</li>
<li><strong>Формула:</strong> <span class="math inline">x_{n+1} = x_n -
\frac{f(x_n)(x_n - x_{n-1})}{f(x_n) - f(x_{n-1})}</span>. Другой вариант
— использование малого шага <span class="math inline">h_n</span>: <span
class="math inline">x_{n+1} = x_n - \frac{h_n f(x_n)}{f(x_n+h_n) -
f(x_n)}</span>.</li>
<li><strong>Скорость:</strong> Сверхлинейная (для метода секущих) или
близкая к квадратичной (при малом <span
class="math inline">h_n</span>).</li>
</ul>
<p><strong>3. Методы высших порядков:</strong></p>
<ul>
<li><strong>Идея:</strong> Использовать больше членов в разложении
Тейлора для аппроксимации функции. Например, квадратичная
аппроксимация.</li>
<li><strong>Формула 3-го порядка:</strong> <span
class="math inline">f(x_n) + f&#39;(x_n)(x_{n+1}-x_n) +
\frac{1}{2}f&#39;&#39;(x_n)(x_{n+1}-x_n)^2 = 0</span>. Используя <span
class="math inline">x_{n+1}-x_n \approx
-\frac{f(x_n)}{f&#39;(x_n)}</span>, получаем: <span
class="math inline">x_{n+1} = x_n - \frac{f(x_n)}{f&#39;(x_n)} -
\frac{1}{2}\frac{f&#39;&#39;(x_n)}{f&#39;(x_n)}\left(\frac{f(x_n)}{f&#39;(x_n)}\right)^2
= x_n - \frac{f(x_n)}{f&#39;(x_n)} - \frac{f(x_n)^2
f&#39;&#39;(x_n)}{2f&#39;(x_n)^3}</span>.</li>
<li><strong>Скорость:</strong> Кубическая. Требует вычисления второй
производной, что еще более трудоемко.</li>
</ul>
<p><strong>4. Метод Ньютона для кратных корней (Метод
Ньютона-Шрёдера):</strong></p>
<ul>
<li><strong>Проблема:</strong> Если корень <span
class="math inline">x^*</span> имеет кратность <span
class="math inline">m &gt; 1</span>, то <span
class="math inline">f&#39;(x^*) = 0</span>, и обычный метод Ньютона
сходится лишь <strong>линейно</strong> со знаменателем <span
class="math inline">(m-1)/m</span>. <span
class="math inline">x_{n+1}-x_* \approx
\frac{m-1}{m}(x_n-x_*)</span>.</li>
<li><strong>Идея:</strong> Модифицировать шаг итерации, умножив его на
известную кратность корня <span class="math inline">m</span>.</li>
<li><strong>Формула:</strong> <span class="math inline">x_{n+1} = x_n -
m\frac{f(x_n)}{f&#39;(x_n)}</span>.</li>
<li><strong>Скорость:</strong> Эта модификация восстанавливает
<strong>квадратичную</strong> скорость сходимости для кратных корней.
<span class="math inline">x_{n+1}-x_* = O((x_n-x_*)^2)</span>.</li>
<li><strong>Недостаток:</strong> Требуется заранее знать кратность корня
<span class="math inline">m</span>.</li>
</ul>
<hr />
<h3
id="методы-решения-снау-систем-нелинейных-алгебраических-уравнений.-метод-простых-итераций.">23.
Методы решения СНАУ (систем нелинейных алгебраических) уравнений. Метод
простых итераций.</h3>
<p>Система нелинейных алгебраических уравнений (СНАУ) имеет вид: <span
class="math display">
\left\{
\begin{aligned}
f_1(x_1, \dots, x_n) &amp;= 0 \\
\dots \\
f_n(x_1, \dots, x_n) &amp;= 0
\end{aligned}
\right.
</span> или в векторной форме <span class="math inline">f(x) = 0</span>,
где <span class="math inline">x = (x_1, \dots, x_n)^T</span> и <span
class="math inline">f = (f_1, \dots, f_n)^T</span>.</p>
<p><strong>Метод простой итерации (МПИ) для СНАУ:</strong></p>
<p>Является обобщением одномерного случая.</p>
<ol type="1">
<li><p><strong>Преобразование системы:</strong> Исходная система <span
class="math inline">f(x) = 0</span> преобразуется к эквивалентному виду:
<span class="math inline">x = \varphi(x)</span> или покомпонентно: <span
class="math display">
\begin{aligned}
x_1 &amp;= \varphi_1(x_1, \dots, x_n) \\
\dots \\
x_n &amp;= \varphi_n(x_1, \dots, x_n)
\end{aligned}
</span></p></li>
<li><p><strong>Итерационный процесс:</strong></p>
<ul>
<li>Выбирается начальное приближение (вектор) <span
class="math inline">x^{(0)}</span>.</li>
<li>Последовательность приближений строится по формуле: <span
class="math inline">x^{(m+1)} = \varphi(x^{(m)})</span></li>
</ul></li>
</ol>
<p><strong>Сходимость МПИ для СНАУ:</strong></p>
<ul>
<li><p><strong>Определение (Выпуклая область):</strong> Область <span
class="math inline">\Omega \subset \mathbb{R}^n</span> называют
<strong>выпуклой</strong>, если наряду с двумя любыми точками <span
class="math inline">a \in \Omega</span> и <span class="math inline">b
\in \Omega</span> она включает все точки отрезка <span
class="math inline">[a,b]</span>.</p></li>
<li><p><strong>Определение (Сжимающее отображение):</strong> Отображение
<span class="math inline">y=\varphi(x)</span> называется
<strong>сжимающим</strong> в замкнутой выпуклой области <span
class="math inline">\Omega</span>, если <span
class="math inline">\exists q: 0 \le q &lt; 1</span> и <span
class="math inline">\rho(\varphi(x_1), \varphi(x_2)) \le q
\rho(x_1,x_2)</span>, <span class="math inline">\forall x_1, x_2 \in
\Omega</span>, где <span class="math inline">\rho(x_1,x_2)</span> —
метрика в <span class="math inline">\mathbb{R}^n</span>.</p></li>
<li><p><strong>Теорема (Принцип сжимающих отображений):</strong> Для
сжимающего отображения <span class="math inline">y=\varphi(x)</span>
уравнение <span class="math inline">x=\varphi(x)</span> имеет
единственное решение <span class="math inline">x_*</span>, и
итерационный процесс <span class="math inline">x^{(m+1)} =
\varphi(x^{(m)})</span> сходится к <span class="math inline">x^*</span>
при любом <span class="math inline">x^{(0)}</span>.</p></li>
<li><p><strong>Достаточное условие сходимости (через матрицу
Якоби):</strong> Пусть область <span class="math inline">\Omega \subset
\mathbb{R}^n</span> выпуклая, <span class="math inline">x \in
\Omega</span>, а компоненты <span
class="math inline">\varphi_i(x)</span> вектор-функции <span
class="math inline">\varphi(x)=(\varphi_1, \dots, \varphi_n)^T</span>
имеют равномерно непрерывные производные первого порядка. Положим, что
норма матрицы Якоби <span class="math inline">J = \frac{d\varphi(x)}{dx}
= \begin{pmatrix} \frac{\partial \varphi_1}{\partial x_1} &amp; \dots
&amp; \frac{\partial \varphi_1}{\partial x_n} \\ \vdots &amp; \ddots
&amp; \vdots \\ \frac{\partial \varphi_n}{\partial x_1} &amp; \dots
&amp; \frac{\partial \varphi_n}{\partial x_n} \end{pmatrix}</span> не
превосходит некоторого числа <span class="math inline">0 \le q &lt;
1</span>, т.е. <span class="math inline">||J|| \le q &lt; 1, \quad
\forall x \in \Omega</span>. Тогда отображение <span
class="math inline">y=\varphi(x)</span> является сжимающим в <span
class="math inline">\Omega</span>, т.е. <span
class="math inline">\rho(\varphi(x_1), \varphi(x_2)) \le q
\rho(x_1,x_2)</span>.</p></li>
</ul>
<p>Как и в одномерном случае, выбор представления <span
class="math inline">x=\varphi(x)</span> и начального приближения <span
class="math inline">x^{(0)}</span> является ключевым для обеспечения
сходимости.</p>
<hr />
<h3
id="метод-ньютона-решения-снау.-его-реализации-и-модификации.-вопрос-о-сходимости-метода-ньютона.">24.
Метод Ньютона решения СНАУ. Его реализации и модификации. Вопрос о
сходимости метода Ньютона.</h3>
<p>Метод Ньютона является обобщением одномерного метода на системы
уравнений и одним из самых эффективных методов решения СНАУ.</p>
<p><strong>Идея метода:</strong> На каждом шаге система нелинейных
функций <span class="math inline">f(x)</span> линеаризуется в
окрестности текущего приближения <span
class="math inline">x^{(m)}</span> с помощью разложения в ряд Тейлора
(ограничиваясь линейными членами). <span class="math inline">f(x)
\approx f(x^{(m)}) + f&#39;(x^{(m)})(x - x^{(m)})</span> Здесь <span
class="math inline">f&#39;(x)</span> — это <strong>матрица
Якоби</strong> <span class="math inline">J(x)</span> системы <span
class="math inline">f(x)</span>. <span class="math display">
J(x) = f&#39;(x) = \frac{df}{dx} = \begin{pmatrix}
\frac{\partial f_1}{\partial x_1} &amp; \dots &amp; \frac{\partial
f_1}{\partial x_n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_n}{\partial x_1} &amp; \dots &amp; \frac{\partial
f_n}{\partial x_n}
\end{pmatrix}
</span></p>
<p><strong>Вывод итерационной формулы:</strong> 1. Линеаризованная
система: <span class="math inline">f(x^{(m)}) + J(x^{(m)})(x^{(m+1)} -
x^{(m)}) = 0</span>. 2. Выражаем поправку <span
class="math inline">\Delta x^{(m)} = x^{(m+1)} - x^{(m)}</span>, решая
СЛАУ: <span class="math inline">J(x^{(m)})\Delta x^{(m)} =
-f(x^{(m)})</span>. 3. Новое приближение: <span
class="math inline">x^{(m+1)} = x^{(m)} + \Delta x^{(m)}</span>. Или, в
терминах обратной матрицы: <span class="math inline">x^{(m+1)} = x^{(m)}
- [J(x^{(m)})]^{-1}f(x^{(m)})</span>.</p>
<p><strong>Реализация одного шага метода Ньютона:</strong> 1. Вычислить
вектор <span class="math inline">f(x^{(m)})</span>. 2. Вычислить матрицу
Якоби <span class="math inline">J(x^{(m)})</span>. 3. Решить СЛАУ <span
class="math inline">J(x^{(m)})\Delta x = -f(x^{(m)})</span> относительно
<span class="math inline">\Delta x</span>. 4. Найти новое приближение
<span class="math inline">x^{(m+1)} = x^{(m)} + \Delta x</span>.</p>
<p><strong>Модификации:</strong> * <strong>Упрощенный метод
Ньютона:</strong> Матрица Якоби и ее обратная (или LU-разложение)
вычисляются только один раз <span class="math inline">J(x^{(0)})</span>.
На последующих шагах решается СЛАУ с той же матрицей. Это снижает
трудоемкость шага, но замедляет сходимость с квадратичной до линейной. *
<strong>Разностный метод Ньютона:</strong> Элементы матрицы Якоби <span
class="math inline">\frac{\partial f_i}{\partial x_j}</span> заменяются
конечно-разностными аппроксимациями.</p>
<p><strong>Вопрос о сходимости:</strong> * <strong>Локальная
сходимость:</strong> Метод Ньютона обладает <strong>квадратичной
скоростью сходимости</strong>, если начальное приближение <span
class="math inline">x^{(0)}</span> выбрано достаточно близко к решению
<span class="math inline">x^*</span> и если матрица Якоби <span
class="math inline">J(x^*)</span> невырождена. *
<strong>Сложность:</strong> Достаточные условия сходимости для СНАУ
имеют сложный вид и на практике их трудно проверить. Сходимость сильно
зависит от выбора <span class="math inline">x^{(0)}</span>. Если <span
class="math inline">x^{(0)}</span> далеко от решения, метод может легко
разойтись.</p>
<hr />
<h3
id="сведение-задачи-решения-слау-и-снау-к-задаче-нахождения-минимума-функционала.-проблема-поиска-минимума-и-различные-подходы-к-ее-решению.">25.
Сведение задачи решения СЛАУ и СНАУ к задаче нахождения минимума
функционала. Проблема поиска минимума и различные подходы к ее
решению.</h3>
<p>Многие задачи решения уравнений можно свести к эквивалентной задаче
поиска экстремума (обычно минимума) некоторой функции или функционала.
Этот подход особенно полезен, когда прямые или итерационные методы
решения уравнений работают плохо.</p>
<p><strong>Сведение СЛАУ к задаче минимизации:</strong> Для СЛАУ <span
class="math inline">Ax=b</span> с <strong>симметричной положительно
определенной</strong> матрицей <span class="math inline">A</span> ее
решение <span class="math inline">x^*</span> является точкой минимума
<strong>квадратичного функционала энергии</strong>: <span
class="math inline">\Phi(x) = (Ax, x) - 2(b, x) + C</span>
<strong>Теорема:</strong> Если <span
class="math inline">A=A^*&gt;0</span>, то существует единственный
элемент <span class="math inline">y \in \mathbb{R}^n</span>, придающий
наименьшее значение квадратичному функционалу <span
class="math inline">\Phi(x)</span>, являющийся решением СЛАУ <span
class="math inline">Ax=b</span>. Доказывается, что условие минимума
<span class="math inline">\text{grad } \Phi(x) = 0</span> эквивалентно
исходному уравнению <span class="math inline">Ax=b</span>, так как <span
class="math inline">\text{grad } \Phi(x) = 2(Ax - b)</span>.</p>
<p><strong>Сведение СНАУ к задаче минимизации:</strong> Для СНАУ <span
class="math inline">f(x)=0</span> можно построить неотрицательную
функцию (функционал), минимум которой равен нулю и достигается в точке,
являющейся решением системы. Стандартный выбор — <strong>сумма квадратов
невязок</strong>: <span class="math inline">\Phi(x) = \sum_{i=1}^n
f_i^2(x)</span> где <span class="math inline">x=(x_1, \dots,
x_n)^T</span>. Очевидно, <span class="math inline">\Phi(x) \ge 0</span>,
и <span class="math inline">\Phi(x^*) = 0</span> тогда и только тогда,
когда <span class="math inline">f_i(x^*) = 0</span> для всех <span
class="math inline">i</span>, то есть когда <span
class="math inline">x^*</span> — решение СНАУ.</p>
<p><strong>Проблема поиска минимума:</strong> Задача нахождения минимума
функции многих переменных <span class="math inline">\Phi(x)</span> сама
по себе является сложной. * <strong>Локальный vs Глобальный
минимум:</strong> Методы обычно находят какой-либо <strong>локальный
минимум</strong>. Гарантировать нахождение <strong>глобального
минимума</strong> сложно. В случае СЛАУ с <span
class="math inline">A&gt;0</span> и СНАУ (с функционалом суммы
квадратов) локальный минимум является и глобальным. *
<strong>“Овраги”:</strong> Поверхность уровня функции <span
class="math inline">\Phi(x)</span> может иметь сложный рельеф с узкими и
вытянутыми впадинами (“оврагами”), что сильно замедляет сходимость
многих методов.</p>
<p><strong>Подходы к решению задачи минимизации:</strong> 1.
<strong>Методы, использующие производные (градиентные):</strong> *
<strong>Метод наискорейшего спуска:</strong> Движение в направлении
антиградиента. * <strong>Метод покоординатного спуска:</strong>
Минимизация по одной координате за раз. 2. <strong>Методы, не
использующие производные (прямого поиска):</strong> * <strong>Метод
перебора (сеточный):</strong> Простой, но очень неэффективный. *
<strong>Методы исключения отрезков (для 1D):</strong> Дихотомия, золотое
сечение.</p>
<hr />
<h3 id="метод-покоординатного-спуска.">26. Метод покоординатного
спуска.</h3>
<p>Метод покоординатного спуска — это итерационный метод минимизации
функции нескольких переменных <span class="math inline">F(x_1, ...,
x_n)</span>.</p>
<p><strong>Идея метода:</strong> Вместо того чтобы двигаться в сложном
многомерном пространстве, на каждом шаге минимизация производится только
вдоль одной координатной оси, при этом значения остальных координат
фиксируются.</p>
<p><strong>Алгоритм:</strong> 1. Выбирается начальное приближение <span
class="math inline">x^{(0)} = (x_1^{(0)}, ..., x_n^{(0)})^T</span>. 2.
Итерационный процесс (один цикл по всем координатам): * Фиксируются
<span class="math inline">x_2^{(0)}, ..., x_n^{(0)}</span>. Находится
<span class="math inline">x_1^{(1)}</span> из условия минимизации
функции одной переменной <span class="math inline">F(x_1, x_2^{(0)},
..., x_n^{(0)})</span>. * Затем, исходя из нового приближения <span
class="math inline">(x_1^{(1)}, x_2^{(0)}, \dots, x_n^{(0)})</span>,
путем минимизации функции <span class="math inline">F(x_1^{(1)}, x_2,
\dots, x_n^{(0)})</span> по переменной <span
class="math inline">x_2</span> находим следующее приближение <span
class="math inline">(x_1^{(1)}, x_2^{(1)}, \dots, x_n^{(0)})</span>. * …
* Процесс циклически повторяется до достижения критерия остановки.</p>
<p><strong>Геометрическая интерпретация:</strong> Движение к минимуму
происходит “ступеньками” параллельно координатным осям. На каждом шаге
мы движемся по прямой, параллельной оси, до точки, где эта прямая
касается линии уровня функции.</p>
<p><strong>Связь с методом Зейделя:</strong> При решении СЛАУ с
симметричной положительно определенной матрицей <span
class="math inline">A</span> через минимизацию функционала <span
class="math inline">\Phi(x) = (Ax, x) - 2(b, x)</span>, метод
покоординатного спуска <strong>в точности совпадает</strong> с методом
Зейделя. Условие минимизации <span class="math inline">\Phi(x)</span> по
<span class="math inline">x_k</span>, <span
class="math inline">\frac{\partial \Phi}{\partial x_k} = 0</span>, с
учетом <span class="math inline">\text{grad } \Phi(x) = 2(Ax-b)</span>
приводит к: <span class="math inline">2 \left(\sum_{j=1}^n a_{kj} x_j -
b_k\right) = 0</span>. Это уравнение соответствует уравнению при
итерациях Зейделя.</p>
<p><strong>Недостатки:</strong> * Может сходиться очень медленно,
особенно для функций с вытянутыми “оврагами”, не ориентированными вдоль
осей.</p>
<hr />
<h3 id="метод-скорейшего-наискорейшего-спуска.">27. Метод скорейшего
(наискорейшего) спуска.</h3>
<p>Метод наискорейшего спуска (или градиентного спуска) — это
классический итерационный метод минимизации функции <span
class="math inline">\Phi(x)</span>.</p>
<p><strong>Идея метода:</strong> На каждом шаге движение к минимуму
осуществляется в направлении, <strong>противоположном направлению
градиента</strong> <span class="math inline">\text{grad }
\Phi(x)</span>, так как градиент указывает направление наибольшего роста
функции.</p>
<p><strong>Алгоритм:</strong> 1. <strong>Основная итерационная
формула:</strong> <span class="math inline">x^{(m+1)} = x^{(m)} - \tau_m
\text{grad } \Phi(x^{(m)})</span>, где <span class="math inline">\tau_m
&gt; 0</span> — <strong>шаг спуска</strong>.</p>
<ol start="2" type="1">
<li><strong>Выбор шага <span class="math inline">\tau_m</span> (метод
наискорейшего спуска):</strong> Шаг <span
class="math inline">\tau_m</span> выбирается на каждой итерации из
условия <strong>оптимальности</strong>: он должен минимизировать функцию
<span class="math inline">\Phi</span> вдоль направления антиградиента.
То есть <span class="math inline">\tau_m</span> находится из решения
одномерной задачи минимизации: <span class="math inline">\min_{\tau_m}
\Phi(x^{(m)} - \tau_m \text{grad } \Phi(x^{(m)}))</span> Это
эквивалентно решению уравнения <span
class="math inline">\frac{d}{d\tau_m} \Phi(x^{(m+1)}(\tau_m)) =
0</span>.</li>
</ol>
<p><strong>Применение к решению СЛАУ:</strong> Для СЛАУ <span
class="math inline">Ax=b</span> с симметричной положительно определенной
матрицей <span class="math inline">A</span> (<span
class="math inline">A=A^*&gt;0</span>), используем функционал <span
class="math inline">\Phi(x)=(Ax,x)-2(b,x)</span>. * <span
class="math inline">\text{grad } \Phi(x) = 2(Ax - b)</span>. Вводя
вектор невязки <span class="math inline">r_m = Ax^{(m)}-b</span>,
итерационная формула записывается как: <span
class="math inline">x^{(m+1)} = x^{(m)} - \tau_m r_m</span>. *
Оптимальный шаг <span class="math inline">\tau_m</span> находится из
условия <span
class="math inline">\frac{d}{d\tau_m}\Phi(x^{(m+1)})=0</span>, которое
приводит к: <span class="math inline">(Ax^{(m+1)}-b, Ax^{(m)}-b) =
0</span>. Подставляя <span class="math inline">x^{(m+1)} = x^{(m)} -
\tau_m(Ax^{(m)}-b)</span>, получаем: <span
class="math inline">((Ax^{(m)}-b) - \tau_m A(Ax^{(m)}-b),
Ax^{(m)}-b)=0</span>. Откуда: <span class="math inline">\tau_m =
\frac{(r_m, r_m)}{(Ar_m, r_m)}</span>.</p>
<p><strong>Свойства:</strong> * Сходимость гарантирована для
симметричных положительно определенных матриц. * Скорость сходимости —
<strong>линейная</strong>. Знаменатель прогрессии <span
class="math inline">\delta_0 = \frac{1-\xi}{1+\xi}</span>, где <span
class="math inline">\xi=\frac{\lambda_{\min}(A)}{\lambda_{\max}(A)}</span>.
* Сходимость может быть очень медленной для плохо обусловленных задач
(<span class="math inline">\mu \gg 1</span>), так как <span
class="math inline">\delta_0</span> будет близко к 1.</p>
<hr />
<h3
id="понятие-переопределенной-системы.-решение-в-смысле-наименьших-квадратов.">28.
Понятие переопределенной системы. Решение в смысле наименьших
квадратов.</h3>
<p><strong>Переопределенная система:</strong> Система линейных
алгебраических уравнений <span class="math inline">Ax=b</span>
называется <strong>переопределенной</strong>, когда число уравнений
<span class="math inline">m</span> больше числа неизвестных <span
class="math inline">n</span> (<span class="math inline">m &gt;
n</span>). <span class="math display">
A = \begin{pmatrix}
a_{11} &amp; \dots &amp; a_{1n} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; \dots &amp; a_{mn}
\end{pmatrix}, \quad
x = \begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix}, \quad
b = \begin{pmatrix}
b_1 \\
\vdots \\
b_m
\end{pmatrix}
</span> <span class="math display">
\left\{
\begin{aligned}
a_{11}x_1 + \dots + a_{1n}x_n &amp;= b_1 \\
\vdots \\
a_{m1}x_1 + \dots + a_{mn}x_n &amp;= b_m
\end{aligned}
\right. \quad (1)
</span> Как правило, такая система <strong>не имеет классического
решения</strong>, то есть не существует вектора <span
class="math inline">x</span>, который бы удовлетворял всем <span
class="math inline">m</span> уравнениям одновременно.</p>
<p><strong>Решение в смысле наименьших квадратов (МНК):</strong> Раз
классического решения нет, ищут “наилучшее” приближенное решение.
<strong>Метод наименьших квадратов (МНК)</strong> предлагает в качестве
такого решения вектор <span class="math inline">x^*</span>, который
минимизирует <strong>сумму квадратов невязок</strong> всех
уравнений.</p>
<ul>
<li><strong>Невязка</strong> <span class="math inline">i</span>-го
уравнения: <span class="math inline">r_i = (a_{i1}x_1 + \dots +
a_{in}x_n) - b_i</span>.</li>
<li><strong>Функционал МНК:</strong> <span class="math inline">\Phi(x_1,
\dots, x_n) = \sum_{i=1}^m r_i^2 = \sum_{i=1}^m (a_{i1}x_1 + \dots +
a_{in}x_n - b_i)^2</span>.</li>
<li>Искомое решение <span class="math inline">x^*</span> — это точка
минимума функции <span class="math inline">\Phi(x)</span>, называемая
<strong>нормальным псевдорешением</strong>.</li>
</ul>
<p><strong>Нахождение решения:</strong> Для нахождения минимума <span
class="math inline">\Phi(x)</span> необходимо приравнять к нулю все её
частные производные по <span class="math inline">x_j</span>: <span
class="math inline">\frac{\partial\Phi(x_1, \dots, x_n)}{\partial x_j} =
0</span>, для <span class="math inline">j=1, \dots, n</span>. Выполнив
дифференцирование, получим: <span class="math inline">2\sum_{i=1}^m
a_{ij}(a_{i1}x_1 + \dots + a_{in}x_n - b_i) = 0</span> Это приводит к
новой системе из <span class="math inline">n</span> линейных уравнений с
<span class="math inline">n</span> неизвестными, которая называется
<strong>системой нормальных уравнений</strong>: <span
class="math display">
\left\{
\begin{aligned}
\left(\sum_{i=1}^m a_{i1}^2\right)x_1 + \dots + \left(\sum_{i=1}^m
a_{i1}a_{in}\right)x_n &amp;= \sum_{i=1}^m b_i a_{i1} \\
\vdots \\
\left(\sum_{i=1}^m a_{in}a_{i1}\right)x_1 + \dots + \left(\sum_{i=1}^m
a_{in}^2\right)x_n &amp;= \sum_{i=1}^m b_i a_{in}
\end{aligned}
\right.
</span> В матричной форме эта система записывается как: <span
class="math inline">A^TAx = A^Tb</span> Решение этой “квадратной”
системы и дает искомое псевдорешение.</p>
<hr />
<h3
id="проблема-поиска-минимума.-постановка-задачи.-метод-дихотомии.-золотое-сечение.">29.
Проблема поиска минимума. Постановка задачи. Метод дихотомии. Золотое
сечение.</h3>
<p><strong>Постановка задачи:</strong> Пусть на множестве <span
class="math inline">U</span> определена скалярная функция (целевая
функция) <span class="math inline">\Phi(u)</span>. Задача состоит в
нахождении элемента <span class="math inline">u^*</span> (или <span
class="math inline">u^{**}</span>), на котором <span
class="math inline">\Phi(u)</span> достигает: * <strong>Локального
минимума:</strong> <span class="math inline">\Phi(u^*) \le
\Phi(u)</span> в некоторой окрестности <span
class="math inline">u^*</span>. * <strong>Глобального минимума:</strong>
<span class="math inline">\Phi(u^{**}) = \inf_{u \in U}
\Phi(u)</span>.</p>
<p>Рассмотрим задачу одномерной минимизации на отрезке <span
class="math inline">[a,b]</span> для <strong>унимодальной</strong>
функции (имеющей на отрезке ровно один минимум).</p>
<p><strong>1. Метод дихотомии (для поиска минимума):</strong></p>
<ul>
<li><strong>Идея:</strong> Исключение части отрезка, где минимума
заведомо нет.</li>
<li><strong>Алгоритм:</strong>
<ol type="1">
<li>Внутри отрезка <span class="math inline">[a,b]</span> выбираются две
близкие к центру точки <span class="math inline">u_1</span> и <span
class="math inline">u_2</span> (<span class="math inline">u_1 &lt;
u_2</span>). Например, <span class="math inline">u_1 =
\frac{b+a-\Delta}{2}</span>, <span class="math inline">u_2 =
\frac{b+a+\Delta}{2}</span>, где <span class="math inline">\Delta</span>
— малое число.</li>
<li>Вычисляются <span class="math inline">\Phi(u_1)</span> и <span
class="math inline">\Phi(u_2)</span>.</li>
<li>Если <span class="math inline">\Phi(u_1) \le \Phi(u_2)</span> —
минимум находится левее <span class="math inline">u_2</span>, новый
отрезок поиска <span class="math inline">[a, u_2]</span>.</li>
<li>Если <span class="math inline">\Phi(u_1) &gt; \Phi(u_2)</span> —
минимум находится правее <span class="math inline">u_1</span>, новый
отрезок поиска <span class="math inline">[u_1, b]</span>.</li>
<li>Процесс повторяется.</li>
</ol></li>
<li><strong>Свойства:</strong> Надежен, но на каждом шаге требует
вычисления функции в двух новых точках. Длина отрезка на <span
class="math inline">n</span> итераций <span class="math inline">\Delta_n
= \frac{b-a}{2^n} + \left(1-\frac{1}{2^n}\right)\Delta</span>. Точность
<span class="math inline">\varepsilon_n =
\frac{\Delta_n}{2}</span>.</li>
</ul>
<p><strong>2. Метод золотого сечения:</strong></p>
<ul>
<li><strong>Идея:</strong> Более эффективный метод исключения отрезков,
который на каждой итерации (кроме первой) требует вычисления функции
<strong>только в одной новой точке</strong>.</li>
<li><strong>Алгоритм:</strong>
<ol type="1">
<li>Отрезок <span class="math inline">[a,b]</span> делится двумя точками
<span class="math inline">x_1</span> и <span
class="math inline">x_2</span> в пропорции <strong>золотого
сечения</strong>: <span
class="math inline">x_1=a+\frac{3-\sqrt{5}}{2}(b-a)</span>, <span
class="math inline">x_2=a+\frac{\sqrt{5}-1}{2}(b-a)</span>. Коэффициент
<span class="math inline">\tau = \frac{\sqrt{5}-1}{2} \approx
0.61803</span>.</li>
<li>Сравниваются <span class="math inline">\Phi(x_1)</span> и <span
class="math inline">\Phi(x_2)</span>.</li>
<li>Если <span class="math inline">\Phi(x_1) \le \Phi(x_2)</span>, новый
отрезок <span class="math inline">[a, x_2]</span>. При этом точка <span
class="math inline">x_1</span> становится “новой” <span
class="math inline">x_2</span> на этом отрезке.</li>
<li>Если <span class="math inline">\Phi(x_1) &gt; \Phi(x_2)</span>,
новый отрезок <span class="math inline">[x_1, b]</span>. Точка <span
class="math inline">x_2</span> становится “новой” <span
class="math inline">x_1</span>.</li>
<li>На каждой итерации длина отрезка поиска умножается на <span
class="math inline">\tau</span>. После <span
class="math inline">n</span> итераций длина становится <span
class="math inline">\Delta_n = \tau^n(b-a)</span>.</li>
</ol></li>
<li><strong>Свойства:</strong> Требует вдвое меньше вычислений функции,
чем дихотомия, что часто делает метод более эффективным на
практике.</li>
</ul>
<hr />
<h3 id="частичная-проблема-собственных-значений.-степенной-метод.">30.
Частичная проблема собственных значений. Степенной метод.</h3>
<p><strong>Частичная проблема собственных значений</strong> состоит в
отыскании одного или нескольких собственных значений матрицы, но не
всех. Чаще всего ищут: * <strong>Максимальное по модулю</strong>
собственное значение <span class="math inline">\lambda_1</span>. *
<strong>Минимальное по модулю</strong> собственное значение <span
class="math inline">\lambda_n</span>. Эти значения важны для анализа
устойчивости, сходимости итерационных методов и оценки числа
обусловленности.</p>
<p><strong>Степенной метод:</strong> Это простой итерационный метод для
нахождения <strong>максимального по модулю</strong> собственного
значения <span class="math inline">\lambda_1</span> (доминантного) и
соответствующего ему собственного вектора <span
class="math inline">x_1</span>.</p>
<p><strong>Условия применимости:</strong> * Матрица <span
class="math inline">A</span> должна иметь <span
class="math inline">n</span> линейно независимых собственных векторов
(быть матрицей простой структуры). * Должно существовать доминантное
собственное значение, то есть <span class="math inline">|\lambda_1| &gt;
|\lambda_2| \ge \dots \ge |\lambda_n|</span>.</p>
<p><strong>Алгоритм:</strong> 1. Выбирается произвольный ненулевой
начальный вектор <span class="math inline">x^{(0)}</span>, который
предполагается не ортогональным собственному вектору <span
class="math inline">x_1</span>. 2. Строится итерационная
последовательность: <span class="math inline">x^{(s+1)} =
Ax^{(s)}</span> 3. <strong>Анализ сходимости:</strong> Разложим <span
class="math inline">x^{(0)}</span> по базису из собственных векторов
<span class="math inline">x_i</span>: <span class="math inline">x^{(0)}
= \sum_{i=1}^n c_i x_i</span>. Тогда <span class="math inline">x^{(s)} =
A^s x^{(0)} = A^s \left(\sum_{i=1}^n c_i x_i\right) = \sum_{i=1}^n c_i
A^s x_i = \sum_{i=1}^n c_i \lambda_i^s x_i = \lambda_1^s \left(c_1 x_1 +
\sum_{i=2}^{n} c_i\left(\frac{\lambda_i}{\lambda_1}\right)^s
x_i\right)</span>. Так как <span
class="math inline">|\frac{\lambda_i}{\lambda_1}| &lt; 1</span> для
<span class="math inline">i &gt; 1</span>, то при <span
class="math inline">s \to \infty</span> слагаемые <span
class="math inline">\left(\frac{\lambda_i}{\lambda_1}\right)^s</span>
стремятся к нулю. <span class="math inline">x^{(s)} \approx
c_1\lambda_1^s x_1</span>. Это означает, что вектор <span
class="math inline">x^{(s)}</span> по направлению сходится к
собственному вектору <span class="math inline">x_1</span>.</p>
<ol start="4" type="1">
<li><strong>Нахождение <span
class="math inline">\lambda_1</span>:</strong> Из <span
class="math inline">x^{(s+1)} \approx \lambda_1 x^{(s)}</span> следует,
что <span class="math inline">\lambda_1</span> можно оценить как
отношение соответствующих компонент: <span class="math inline">\lambda_1
\approx \frac{x_j^{(s+1)}}{x_j^{(s)}}</span>. Более устойчивая оценка
(используя евклидову норму): <span class="math inline">|\lambda_1|
\approx \sqrt{\frac{(x^{(s+1)},x^{(s+1)})}{(x^{(s)},x^{(s)})}}</span>.
Процесс сходится линейно со знаменателем <span
class="math inline">q=|\lambda_2/\lambda_1|</span>. При расчетах на ЭВМ
после каждой итерации может потребоваться нормировать вектор <span
class="math inline">x^{(s+1)}</span>, чтобы избежать переполнений.</li>
</ol>
<p><strong>Нахождение <span class="math inline">\lambda_n</span>
(минимального по модулю):</strong> Если <span
class="math inline">A</span> невырождена, то её собственные значения
<span class="math inline">\lambda(A)</span> и собственные значения <span
class="math inline">A^{-1}</span> связаны как <span
class="math inline">\lambda(A^{-1}) = 1/\lambda(A)</span>. Максимальное
по модулю собственное значение <span class="math inline">A^{-1}</span>
будет <span class="math inline">1/\lambda_n</span>. Таким образом, чтобы
найти <span class="math inline">\lambda_n</span>, можно применить
степенной метод для поиска максимального собственного числа к матрице
<span class="math inline">A^{-1}</span>. Итерационный процесс <span
class="math inline">y^{(s+1)} = A^{-1}y^{(s)}</span> эквивалентен
решению СЛАУ <span class="math inline">Ay^{(s+1)} = y^{(s)}</span> на
каждом шаге.</p>
<hr />
<h3
id="полная-проблема-нахождения-собственных-значений.-метод-lu.-понятие-о-методе-qr.">31.
Полная проблема нахождения собственных значений. Метод LU. Понятие о
методе QR.</h3>
<p><strong>Полная проблема собственных значений</strong> заключается в
нахождении <strong>всех</strong> собственных значений (и, возможно,
собственных векторов) матрицы <span class="math inline">A</span>.</p>
<p><strong>Общая идея итерационных методов:</strong> Основана на
<strong>преобразованиях подобия</strong>. Матрица <span
class="math inline">A</span> с помощью последовательности преобразований
<span class="math inline">A_{n+1} = C_n^{-1}A_nC_n</span> приводится к
более простому виду (треугольному или диагональному), у которого
собственные значения легко находятся (это диагональные элементы). Так
как преобразование подобия сохраняет спектр, найденные числа будут
собственными значениями исходной матрицы <span
class="math inline">A</span>.</p>
<p><strong>1. LU-алгоритм:</strong></p>
<ul>
<li><strong>Идея:</strong> Использует LU-разложение для построения
последовательности подобных матриц.</li>
<li><strong>Алгоритм:</strong>
<ol type="1">
<li>Начальная матрица <span class="math inline">A_0 = A</span>.</li>
<li>На <span class="math inline">n</span>-м шаге:
<ul>
<li>Выполняется LU-разложение: <span class="math inline">A_n =
L_nU_n</span>.</li>
<li>Матрицы <span class="math inline">L_n</span> и <span
class="math inline">U_n</span> перемножаются в обратном порядке для
получения следующей матрицы: <span class="math inline">A_{n+1} =
U_nL_n</span>.</li>
</ul></li>
</ol></li>
<li><strong>Свойства:</strong>
<ul>
<li>Матрица <span class="math inline">A_{n+1}</span> подобна <span
class="math inline">A_n</span> (<span class="math inline">A_{n+1} =
U_nL_n = L_n^{-1}A_nL_n</span>), а значит, все матрицы в
последовательности <span class="math inline">\{A_n\}</span> имеют тот же
спектр, что и <span class="math inline">A</span>.</li>
<li>При ряде ограничений на матрицу <span class="math inline">A</span>
(простейшим из которых является, в частности, требование, чтобы все ее
собственные числа были различны по модулю), последовательность <span
class="math inline">A_n</span> сходится к <strong>верхней треугольной
матрице</strong>.</li>
<li>Собственные значения исходной матрицы <span
class="math inline">A</span> будут стоять на диагонали предельной
треугольной матрицы.</li>
</ul></li>
<li><strong>Недостатки:</strong> Метод может быть численно
неустойчивым.</li>
</ul>
<p><strong>2. QR-алгоритм:</strong></p>
<ul>
<li><strong>Идея:</strong> Аналогичен LU-алгоритму, но использует
численно более устойчивое <strong>QR-разложение</strong>, где <span
class="math inline">Q</span> — ортогональная матрица (<span
class="math inline">Q^{-1}=Q^T</span>), а <span
class="math inline">R</span> — верхняя треугольная.</li>
<li><strong>Теоретическая основа:</strong> <strong>Теорема
Шура</strong>: Для любой вещественной <span class="math inline">n \times
n</span> матрицы <span class="math inline">A</span> найдется такая
вещественная ортогональная <span class="math inline">n \times n</span>
матрица <span class="math inline">Q</span>, что <span
class="math inline">Q^TAQ</span> является квазитреугольной матрицей:
<span class="math display">
Q^TAQ =
\begin{pmatrix}
R_{11} &amp; R_{12} &amp; \dots &amp; R_{1m} \\
0 &amp; R_{22} &amp; \dots &amp; R_{2m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; R_{mm}
\end{pmatrix},
</span> где <span class="math inline">R_{ii}</span> - либо вещественное
число, либо <span class="math inline">2 \times 2</span> матрица с
комплексно сопряженными собственными значениями.</li>
<li><strong>Алгоритм:</strong>
<ol type="1">
<li><span class="math inline">A_0 = A</span>.</li>
<li>На <span class="math inline">n</span>-м шаге:
<ul>
<li>Выполняется QR-разложение: <span class="math inline">A_n =
Q_nR_n</span>.</li>
<li><code>A_{n+1} = R_nQ_n</code>.</li>
</ul></li>
</ol></li>
<li><strong>Свойства:</strong>
<ul>
<li><span class="math inline">A_{n+1}</span> ортогонально подобна <span
class="math inline">A_n</span> (<span class="math inline">A_{n+1} =
R_nQ_n = Q_n^TA_nQ_n</span>).</li>
<li>Последовательность <span class="math inline">A_n</span> сходится к
<strong>квазитреугольной (блочно-треугольной) матрице</strong>.</li>
<li>Действительные собственные значения будут на диагонали.</li>
<li>Пары комплексно-сопряженных собственных чисел будут собственными
значениями диагональных блоков <span class="math inline">2 \times
2</span>.</li>
<li>Скорость сходимости зависит от отношений <span
class="math inline">|\lambda_i|/|\lambda_j|</span>.</li>
</ul></li>
<li><strong>Преимущества:</strong> QR-алгоритм является одним из самых
надежных и широко используемых методов для решения полной проблемы
собственных значений благодаря своей численной устойчивости.</li>
</ul>
</body>
</html>
